import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import io
import warnings
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.metrics import silhouette_score
from scipy.stats import linregress

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# ============================================================================
# é¡µé¢é…ç½®
# ============================================================================

# ============================================================================
# è‡ªå®šä¹‰CSSæ ·å¼
# ============================================================================
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
    }
    .task-section {
        background-color: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid #007bff;
        margin: 1rem 0;
    }
    .status-completed {
        background-color: #d4edda;
        padding: 0.5rem;
        border-radius: 5px;
        margin: 0.2rem 0;
        border-left: 4px solid #28a745;
    }
    .status-pending {
        background-color: #fff3cd;
        padding: 0.5rem;
        border-radius: 5px;
        margin: 0.2rem 0;
        border-left: 4px solid #ffc107;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
    }
    .fix-note {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 5px;
        border-left: 4px solid #1976d2;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)


# ============================================================================
# Session State åˆå§‹åŒ–
# ============================================================================
def initialize_session_state():
    default_states = {
        'raw_data': None,  # åŸå§‹æ•°æ®
        'step1_missing_data': None,  # æ­¥éª¤1ï¼šç¼ºå¤±å€¼ç»Ÿè®¡ç»“æœ
        'step2_price_data': None,  # æ­¥éª¤2ï¼šè¿›è´§ä»·æ ¼å¤„ç†åæ•°æ®
        'step3_profit_data': None,  # æ­¥éª¤3ï¼šåˆ©æ¶¦ä¿®æ­£åæ•°æ®
        'step4_abnormal_data': None,  # æ­¥éª¤4ï¼šå¼‚å¸¸ä¿®æ­£åŠåˆ©æ¶¦é‡ç®—åæ•°æ®
        'step5_minmax_data': None,  # æ­¥éª¤5ï¼šMinMaxæ ‡å‡†åŒ–åæ•°æ®
        'step5_zscore_data': None,  # æ­¥éª¤5ï¼šZScoreæ ‡å‡†åŒ–åæ•°æ®
        'processed_data': None,  # æœ€ç»ˆå¤„ç†æ•°æ®
        'category_encoder': None,  # åˆ†ç±»å˜é‡ç¼–ç å™¨
        'current_file': None,
        'task1_completed': False,
        'task2_completed': False,
        'task3_completed': False,
        'task4_completed': False,
        'column_types': None  # å­—æ®µç±»å‹
    }

    for key, value in default_states.items():
        if key not in st.session_state:
            st.session_state[key] = value


initialize_session_state()


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================
def auto_detect_column_types(df):
    """è‡ªåŠ¨è¯†åˆ«å­—æ®µç±»å‹ï¼šæ•°å€¼å‹ã€æœ‰åºåˆ†ç±»ã€æ— åºåˆ†ç±»ã€æ ‡è¯†å‹"""
    column_types = {
        'numeric': [],  # æ•°å€¼å‹ï¼ˆéœ€æ ‡å‡†åŒ–ï¼‰
        'ordinal': [],  # æœ‰åºåˆ†ç±»
        'nominal': [],  # æ— åºåˆ†ç±»
        'identifier': []  # æ ‡è¯†å‹
    }

    # æ ‡è¯†å‹å­—æ®µè§„åˆ™ï¼šå”¯ä¸€å€¼å æ¯”>80% æˆ– å­—æ®µååŒ…å«"ID/è®¢å•å·/æ—¥æœŸ"
    id_keywords = ['id', 'è®¢å•å·', 'æ—¥æœŸ', 'ç¼–å·', 'åºå·']
    for col in df.columns:
        col_lower = col.lower()
        if any(keyword in col_lower for keyword in id_keywords) or (df[col].nunique() / len(df) > 0.8):
            column_types['identifier'].append(col)
            continue

    # æ•°å€¼å‹å­—æ®µ
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    column_types['numeric'] = [col for col in numeric_cols if col not in column_types['identifier']]

    # åˆ†ç±»å­—æ®µï¼ˆéæ•°å€¼ã€éæ ‡è¯†ï¼‰
    categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
    categorical_cols = [col for col in categorical_cols if col not in column_types['identifier']]

    # åŒºåˆ†æœ‰åº/æ— åºåˆ†ç±»
    ordinal_keywords = ['ç­‰çº§', 'å¹´é¾„', 'è¯„åˆ†', 'æ®µä½', 'å±‚æ¬¡']
    for col in categorical_cols:
        if any(keyword in col for keyword in ordinal_keywords):
            column_types['ordinal'].append(col)
        else:
            column_types['nominal'].append(col)

    return column_types


def clean_numeric_columns(df):
    """æ¸…æ´—æ•°å€¼åˆ—ä¸­çš„éæ•°å€¼å­—ç¬¦"""
    df_clean = df.copy()

    # å°è¯•è¯†åˆ«ä»·æ ¼ç›¸å…³å­—æ®µ
    price_keywords = ['ä»·æ ¼', 'å”®ä»·', 'é‡‘é¢', 'é”€å”®é¢', 'åˆ©æ¶¦', 'æˆæœ¬']
    price_cols = [col for col in df.columns if any(kw in col for kw in price_keywords)]

    # æ¸…æ´—ä»·æ ¼ç›¸å…³å­—æ®µ
    for col in price_cols:
        if df_clean[col].dtype == 'object':
            # å»é™¤å¸¸è§éæ•°å€¼å­—ç¬¦
            df_clean[col] = df_clean[col].astype(str) \
                .str.replace(r'[^\d.]', '', regex=True)
            # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')

    # å¤„ç†ç™¾åˆ†æ¯”å­—æ®µ
    percent_keywords = ['ç‡', 'ç™¾åˆ†æ¯”', 'å æ¯”']
    percent_cols = [col for col in df.columns if any(kw in col for kw in percent_keywords)]
    for col in percent_cols:
        if df_clean[col].dtype == 'object':
            df_clean[col] = df_clean[col].astype(str) \
                .str.replace(r'[%]', '', regex=True)
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce') / 100

    return df_clean


def process_categorical_variables(df, column_types, fit_encoder=True):
    """å¤„ç†åˆ†ç±»å˜é‡ï¼šæœ‰åºâ†’åºæ•°ç¼–ç ï¼Œæ— åºâ†’ç‹¬çƒ­ç¼–ç """
    df_processed = df.copy()
    encoders = {}

    # 1. æœ‰åºåˆ†ç±»ï¼šåºæ•°ç¼–ç 
    if column_types['ordinal'] and fit_encoder:
        ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
        df_ordinal = ordinal_encoder.fit_transform(df_processed[column_types['ordinal']])
        df_ordinal = pd.DataFrame(
            df_ordinal,
            columns=[f"{col}_ç¼–ç " for col in column_types['ordinal']],
            index=df_processed.index
        )
        df_processed = pd.concat([df_processed, df_ordinal], axis=1)
        encoders['ordinal'] = ordinal_encoder

    # 2. æ— åºåˆ†ç±»ï¼šç‹¬çƒ­ç¼–ç 
    if column_types['nominal'] and fit_encoder:
        onehot_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
        df_onehot = onehot_encoder.fit_transform(df_processed[column_types['nominal']])
        # ç”Ÿæˆç‹¬çƒ­ç¼–ç å­—æ®µå
        feature_names = []
        for i, col in enumerate(column_types['nominal']):
            categories = onehot_encoder.categories_[i][1:]  # è·³è¿‡ç¬¬ä¸€ä¸ªç±»åˆ«
            feature_names.extend([f"{col}_{cat}" for cat in categories])
        df_onehot = pd.DataFrame(
            df_onehot,
            columns=feature_names,
            index=df_processed.index
        )
        df_processed = pd.concat([df_processed, df_onehot], axis=1)
        encoders['onehot'] = onehot_encoder
        encoders['onehot_features'] = feature_names

    # 3. éæ‹Ÿåˆæ¨¡å¼
    if not fit_encoder and st.session_state.category_encoder:
        encoders = st.session_state.category_encoder
        if column_types['ordinal']:
            df_ordinal = encoders['ordinal'].transform(df_processed[column_types['ordinal']])
            df_ordinal = pd.DataFrame(
                df_ordinal,
                columns=[f"{col}_ç¼–ç " for col in column_types['ordinal']],
                index=df_processed.index
            )
            df_processed = pd.concat([df_processed, df_ordinal], axis=1)
        if column_types['nominal']:
            df_onehot = encoders['onehot'].transform(df_processed[column_types['nominal']])
            df_onehot = pd.DataFrame(
                df_onehot,
                columns=encoders['onehot_features'],
                index=df_processed.index
            )
            df_processed = pd.concat([df_processed, df_onehot], axis=1)

    return df_processed, encoders


# ============================================================================
# ä»»åŠ¡1ï¼šæ•°æ®é¢„å¤„ç†ç±»ï¼ˆæŒ‰è®ºæ–‡è¦æ±‚ç”Ÿæˆæ ‡å‡†åŒ–è¾“å‡ºæ–‡ä»¶ï¼‰
# ============================================================================
# ============================================================================
# ä»»åŠ¡1ï¼šæ•°æ®é¢„å¤„ç†ç±»ï¼ˆæŒ‰è®ºæ–‡è¦æ±‚ç”Ÿæˆæ ‡å‡†åŒ–è¾“å‡ºæ–‡ä»¶ï¼‰- åŸºäºæºä»£ç é‡æ„
# ============================================================================
class Task1Preprocessor:
    def __init__(self, df):
        self.df = df.copy()
        self.results = {}
        self.column_types = None

    def step1_missing_value_analysis(self):
        """æ­¥éª¤1: ç¼ºå¤±å€¼ç»Ÿè®¡åˆ†æï¼ˆç”Ÿæˆç”µå•† æ­¥éª¤1 ç¼ºå¤±å€¼ç»Ÿè®¡ç»“æœ.xlsxï¼‰"""
        # è®¡ç®—ç¼ºå¤±å€¼ç»Ÿè®¡
        rows = len(self.df)
        missing_stats = []

        for col in self.df.columns:
            non_null_count = self.df[col].count()
            missing_count = rows - non_null_count
            missing_rate = (missing_count / rows) * 100

            missing_stats.append({
                'å­—æ®µå': col,
                'æ•°æ®ç±»å‹': str(self.df[col].dtype),
                'éç©ºå€¼æ•°é‡': non_null_count,
                'ç¼ºå¤±å€¼æ•°é‡': missing_count,
                'ç¼ºå¤±æ¯”ä¾‹%': round(missing_rate, 2)
            })

        missing_df = pd.DataFrame(missing_stats)
        self.results['step1_missing_stats'] = missing_df
        return missing_df

    def step2_price_processing(self, missing_stats):
        """æ­¥éª¤2: è¿›è´§ä»·æ ¼å¤„ç†ï¼ˆç”Ÿæˆç”µå•† æ­¥éª¤2 è¿›è´§ä»·æ ¼å¤„ç†åæ•°æ®.xlsxï¼‰"""
        import re

        df_step2 = self.df.copy()

        # å¤„ç†è¿›è´§ä»·æ ¼å­—æ®µï¼ˆåŸºäºæºä»£ç é€»è¾‘ï¼‰
        if 'è¿›è´§ä»·æ ¼' in df_step2.columns:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤éæ•°å­—å’Œéå°æ•°ç‚¹å­—ç¬¦ï¼Œè½¬æ¢ä¸ºæ•°å€¼å‹
            df_step2['è¿›è´§ä»·æ ¼'] = df_step2['è¿›è´§ä»·æ ¼'].apply(
                lambda x: float(re.sub(r'[^\d\.]', '', str(x))) if re.search(r'[\d\.]', str(x)) else None
            )
            # è½¬æ¢ä¸ºæ•´æ•°å‹ï¼ˆè‹¥å­˜åœ¨å°æ•°ï¼Œå››èˆäº”å…¥ï¼‰
            df_step2['è¿›è´§ä»·æ ¼'] = df_step2['è¿›è´§ä»·æ ¼'].round().astype('Int64')  # ä½¿ç”¨Int64æ”¯æŒç¼ºå¤±å€¼

        self.results['step2_processed'] = df_step2
        return df_step2

    def step3_profit_correction(self, df_step2):
        """æ­¥éª¤3: åˆ©æ¶¦ä¿®æ­£ï¼ˆç”Ÿæˆç”µå•† æ­¥éª¤3 åˆ©æ¶¦ä¿®æ­£åæ•°æ®.xlsxï¼‰"""
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.neighbors import KNeighborsRegressor
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import mean_squared_error

        df_step3 = df_step2.copy()

        # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
        required_cols = ['å®é™…å”®ä»·', 'è¿›è´§ä»·æ ¼', 'é”€å”®æ•°', 'åˆ©æ¶¦']
        missing_cols = [col for col in required_cols if col not in df_step3.columns]
        if missing_cols:
            st.warning(f"åˆ©æ¶¦ä¿®æ­£éœ€ä»¥ä¸‹å­—æ®µï¼š{missing_cols}ï¼Œæ•°æ®ä¸­ç¼ºå¤±ï¼Œè·³è¿‡åˆ©æ¶¦ä¿®æ­£")
            return df_step3

        # è®¡ç®—ç†è®ºåˆ©æ¶¦
        df_step3['ç†è®ºåˆ©æ¶¦'] = (df_step3['å®é™…å”®ä»·'] - df_step3['è¿›è´§ä»·æ ¼']) * df_step3['é”€å”®æ•°']
        # ç­›é€‰é”™è¯¯å’Œæ­£ç¡®æ•°æ®
        error_data = df_step3[df_step3['åˆ©æ¶¦'] != df_step3['ç†è®ºåˆ©æ¶¦']].copy()
        correct_data = df_step3[df_step3['åˆ©æ¶¦'] == df_step3['ç†è®ºåˆ©æ¶¦']].copy()

        st.info(f"åˆ©æ¶¦è®¡ç®—é”™è¯¯æ•°æ®æ¡æ•°ï¼š{len(error_data)}")
        st.info(f"åˆ©æ¶¦è®¡ç®—æ­£ç¡®æ•°æ®æ¡æ•°ï¼ˆè®­ç»ƒæ•°æ®ï¼‰ï¼š{len(correct_data)}")

        if len(correct_data) == 0:
            st.warning("æ— åˆ©æ¶¦è®¡ç®—æ­£ç¡®çš„æ•°æ®ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹è¿›è¡Œè¡¥æ’ï¼Œè·³è¿‡åˆ©æ¶¦ä¿®æ­£")
            df_step3 = df_step3.drop(columns='ç†è®ºåˆ©æ¶¦')
            return df_step3

        # å‡†å¤‡æ¨¡å‹è®­ç»ƒæ•°æ®
        features = ['å®é™…å”®ä»·', 'è¿›è´§ä»·æ ¼', 'é”€å”®æ•°']
        X = correct_data[features]
        y = correct_data['åˆ©æ¶¦']

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # 1. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_model.fit(X_train, y_train)
        # è¯„ä¼°éšæœºæ£®æ—æ¨¡å‹
        rf_pred_test = rf_model.predict(X_test)
        rf_mse = mean_squared_error(y_test, rf_pred_test)

        # 2. è®­ç»ƒKNNæ¨¡å‹
        knn_model = KNeighborsRegressor(n_neighbors=5)
        knn_model.fit(X_train, y_train)
        # è¯„ä¼°KNNæ¨¡å‹
        knn_pred_test = knn_model.predict(X_test)
        knn_mse = mean_squared_error(y_test, knn_pred_test)

        # é€‰æ‹©MSEè¾ƒå°çš„æ¨¡å‹è¿›è¡Œåˆ©æ¶¦è¡¥æ’
        if rf_mse <= knn_mse:
            st.info(f"é€‰æ‹©éšæœºæ£®æ—æ¨¡å‹è¿›è¡Œåˆ©æ¶¦è¡¥æ’ (MSE: {rf_mse:.2f})")
            if len(error_data) > 0:
                error_X = error_data[features]
                pred_error = rf_model.predict(error_X)
                # æ•°æ®ç±»å‹è½¬æ¢
                pred_error = pred_error.round().astype(df_step3['åˆ©æ¶¦'].dtype)
                # é‡ç½®ç´¢å¼•ç¡®ä¿å¯¹é½
                df_step3 = df_step3.reset_index(drop=True)
                error_data = error_data.reset_index(drop=True)
                # æ›´æ–°é”™è¯¯åˆ©æ¶¦å€¼ï¼ˆä¿æŒåˆ—åä¸º"åˆ©æ¶¦"ï¼‰
                df_step3.loc[error_data.index, 'åˆ©æ¶¦'] = pred_error
        else:
            st.info(f"é€‰æ‹©KNNæ¨¡å‹è¿›è¡Œåˆ©æ¶¦è¡¥æ’ (MSE: {knn_mse:.2f})")
            if len(error_data) > 0:
                error_X = error_data[features]
                pred_error = knn_model.predict(error_X)
                # æ•°æ®ç±»å‹è½¬æ¢
                pred_error = pred_error.round().astype(df_step3['åˆ©æ¶¦'].dtype)
                # é‡ç½®ç´¢å¼•ç¡®ä¿å¯¹é½
                df_step3 = df_step3.reset_index(drop=True)
                error_data = error_data.reset_index(drop=True)
                # æ›´æ–°é”™è¯¯åˆ©æ¶¦å€¼ï¼ˆä¿æŒåˆ—åä¸º"åˆ©æ¶¦"ï¼‰
                df_step3.loc[error_data.index, 'åˆ©æ¶¦'] = pred_error

        # åˆ é™¤ä¸´æ—¶çš„ç†è®ºåˆ©æ¶¦åˆ—
        if 'ç†è®ºåˆ©æ¶¦' in df_step3.columns:
            df_step3 = df_step3.drop(columns='ç†è®ºåˆ©æ¶¦')

        self.results['step3_processed'] = df_step3
        return df_step3

    def step4_abnormal_correction(self, df_step3):
        """æ­¥éª¤4: å¼‚å¸¸å€¼ä¿®æ­£åŠåˆ©æ¶¦é‡ç®—ï¼ˆç”Ÿæˆç”µå•† æ­¥éª¤4 å¼‚å¸¸ä¿®æ­£åŠåˆ©æ¶¦é‡ç®—åæ•°æ®.xlsxï¼‰"""
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.neighbors import KNeighborsRegressor
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import mean_squared_error

        df_step4 = df_step3.copy()

        # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
        required_cols = ['å®é™…å”®ä»·', 'è¿›è´§ä»·æ ¼', 'é”€å”®æ•°', 'å®¢æˆ·å¹´é¾„']
        missing_cols = [col for col in required_cols if col not in df_step4.columns]
        if missing_cols:
            st.warning(f"å¼‚å¸¸ä¿®æ­£éœ€ä»¥ä¸‹å­—æ®µï¼š{missing_cols}ï¼Œæ•°æ®ä¸­ç¼ºå¤±ï¼Œè·³è¿‡å¼‚å¸¸ä¿®æ­£")
            return df_step4

        # æ ‡è®°å¼‚å¸¸æ•°æ®ï¼ˆå®é™…å”®ä»· < è¿›è´§ä»·æ ¼ï¼‰
        abnormal_mask = df_step4['å®é™…å”®ä»·'] < df_step4['è¿›è´§ä»·æ ¼']
        abnormal_data = df_step4[abnormal_mask].copy()
        normal_data = df_step4[~abnormal_mask].copy()

        st.info(f"æˆæœ¬é«˜äºå”®ä»·çš„å¼‚å¸¸æ•°æ®æ¡æ•°ï¼š{len(abnormal_data)}")
        st.info(f"æ­£å¸¸æ•°æ®æ¡æ•°ï¼ˆè®­ç»ƒæ•°æ®ï¼‰ï¼š{len(normal_data)}")

        if len(normal_data) == 0:
            st.warning("æ— æ­£å¸¸å”®ä»·æ•°æ®ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹è¿›è¡Œå¼‚å¸¸ä¿®æ­£ï¼Œè·³è¿‡å¼‚å¸¸ä¿®æ­£")
            return df_step4

        # å‡†å¤‡æ¨¡å‹è®­ç»ƒæ•°æ®ï¼ˆé¢„æµ‹åˆç†å®é™…å”®ä»·ï¼‰
        features = ['è¿›è´§ä»·æ ¼', 'é”€å”®æ•°', 'å®¢æˆ·å¹´é¾„']
        target = 'å®é™…å”®ä»·'
        X = normal_data[features]
        y = normal_data[target]

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # 1. è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_model.fit(X_train, y_train)
        # è¯„ä¼°éšæœºæ£®æ—æ¨¡å‹
        rf_pred_test = rf_model.predict(X_test)
        rf_mse = mean_squared_error(y_test, rf_pred_test)

        # 2. è®­ç»ƒKNNæ¨¡å‹
        knn_model = KNeighborsRegressor(n_neighbors=5)
        knn_model.fit(X_train, y_train)
        # è¯„ä¼°KNNæ¨¡å‹
        knn_pred_test = knn_model.predict(X_test)
        knn_mse = mean_squared_error(y_test, knn_pred_test)

        # ç»¼åˆä¸¤ç§æ¨¡å‹ç»“æœè¿›è¡Œå”®ä»·è¡¥æ’ï¼ˆå–å¹³å‡å€¼ï¼‰
        if len(abnormal_data) > 0:
            abnormal_X = abnormal_data[features]
            rf_pred_abnormal = rf_model.predict(abnormal_X)
            knn_pred_abnormal = knn_model.predict(abnormal_X)
            combined_pred = (rf_pred_abnormal + knn_pred_abnormal) / 2
            # æ•°æ®ç±»å‹è½¬æ¢ï¼ˆç¡®ä¿ä¸åŸå”®ä»·å­—æ®µä¸€è‡´ï¼‰
            combined_pred = combined_pred.round().astype(df_step4[target].dtype)
            # æ›´æ–°å¼‚å¸¸æ•°æ®çš„å”®ä»·
            df_step4.loc[abnormal_mask, target] = combined_pred

        # äºŒæ¬¡æ£€æŸ¥å‰©ä½™å¼‚å¸¸ï¼ˆè‹¥ä»æœ‰å”®ä»·<è¿›è´§ä»·ï¼Œå°†å”®ä»·è®¾ä¸ºè¿›è´§ä»·ï¼‰
        remaining_abnormal_mask = df_step4['å®é™…å”®ä»·'] < df_step4['è¿›è´§ä»·æ ¼']
        if remaining_abnormal_mask.sum() > 0:
            st.info(f"äºŒæ¬¡æ£€æŸ¥å‘ç°{remaining_abnormal_mask.sum()}æ¡å‰©ä½™å¼‚å¸¸æ•°æ®ï¼Œå°†å”®ä»·è®¾ä¸ºè¿›è´§ä»·")
            df_step4.loc[remaining_abnormal_mask, 'å®é™…å”®ä»·'] = df_step4.loc[remaining_abnormal_mask, 'è¿›è´§ä»·æ ¼']

        # é‡æ–°è®¡ç®—æ­£ç¡®åˆ©æ¶¦ï¼ˆä¿æŒåˆ—åä¸º"åˆ©æ¶¦"ï¼‰
        df_step4['åˆ©æ¶¦'] = (df_step4['å®é™…å”®ä»·'] - df_step4['è¿›è´§ä»·æ ¼']) * df_step4['é”€å”®æ•°']

        self.results['step4_processed'] = df_step4
        return df_step4

    def step5_standardization(self, df_step4):
        """æ­¥éª¤5: æ ‡å‡†åŒ–å¤„ç†ï¼ˆç”Ÿæˆç”µå•† æ­¥éª¤5 MinMaxæ ‡å‡†åŒ–åæ•°æ®.xlsxå’ŒZScoreæ ‡å‡†åŒ–åæ•°æ®.xlsxï¼‰"""
        from sklearn.preprocessing import StandardScaler, MinMaxScaler

        df_original = df_step4.copy()

        # å®šä¹‰éœ€æ ‡å‡†åŒ–çš„æ•°å€¼åˆ—ï¼ˆåŸºäºæºä»£ç é€»è¾‘ï¼‰
        required_cols = ["è¿›è´§ä»·æ ¼", "å®é™…å”®ä»·", "é”€å”®æ•°", "åˆ©æ¶¦"]
        # è‹¥å­˜åœ¨é”€å”®é¢åˆ—ï¼ŒåŠ å…¥æ ‡å‡†åŒ–èŒƒå›´
        if "é”€å”®é¢" in df_original.columns:
            required_cols.append("é”€å”®é¢")

        # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        missing_cols = [col for col in required_cols if col not in df_original.columns]
        if missing_cols:
            st.warning(f"æ ‡å‡†åŒ–éœ€ä»¥ä¸‹å­—æ®µï¼š{missing_cols}ï¼Œæ•°æ®ä¸­ç¼ºå¤±ï¼Œè·³è¿‡æ ‡å‡†åŒ–")
            return df_original, df_original

        # ç­›é€‰æ•°å€¼å‹åˆ—ï¼ˆæ’é™¤éæ•°å€¼æ•°æ®ï¼‰
        numeric_cols = [col for col in required_cols if pd.api.types.is_numeric_dtype(df_original[col])]
        if not numeric_cols:
            st.warning("æ— å¯ç”¨çš„æ•°å€¼å‹åˆ—è¿›è¡Œæ ‡å‡†åŒ–")
            return df_original, df_original

        st.info(f"å¾…æ ‡å‡†åŒ–çš„æ•°å€¼åˆ—ï¼š{numeric_cols}")

        # 1. Z-Scoreæ ‡å‡†åŒ–ï¼ˆå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1ï¼‰
        zscore_scaler = StandardScaler()
        df_zscore = df_original.copy()
        df_zscore[numeric_cols] = zscore_scaler.fit_transform(df_zscore[numeric_cols])

        # 2. Min-Maxæ ‡å‡†åŒ–ï¼ˆèŒƒå›´0-1ï¼‰
        minmax_scaler = MinMaxScaler(feature_range=(0, 1))
        df_minmax = df_original.copy()
        df_minmax[numeric_cols] = minmax_scaler.fit_transform(df_minmax[numeric_cols])

        self.results['step5_minmax'] = df_minmax
        self.results['step5_zscore'] = df_zscore
        self.results['numeric_cols'] = numeric_cols

        return df_minmax, df_zscore

    def generate_all_results(self):
        """ç”Ÿæˆæ‰€æœ‰æ­¥éª¤çš„ç»“æœï¼ˆæŒ‰è®ºæ–‡è¦æ±‚çš„æ–‡ä»¶æ ¼å¼ï¼‰"""
        try:
            # æ‰§è¡Œå…¨æµç¨‹æ­¥éª¤
            step1_missing = self.step1_missing_value_analysis()
            step2_price = self.step2_price_processing(step1_missing)
            step3_profit = self.step3_profit_correction(step2_price)
            step4_abnormal = self.step4_abnormal_correction(step3_profit)
            step5_minmax, step5_zscore = self.step5_standardization(step4_abnormal)

            # å­—æ®µç±»å‹è¯†åˆ«
            self.column_types = auto_detect_column_types(step4_abnormal)

            # å¤„ç†åˆ†ç±»å˜é‡
            final_data, encoders = process_categorical_variables(
                step4_abnormal, self.column_types, fit_encoder=True)

            # æ•´ç†ç»“æœæ–‡ä»¶
            result_files = {
                'ç”µå•† æ­¥éª¤1 ç¼ºå¤±å€¼ç»Ÿè®¡ç»“æœ.xlsx': step1_missing,
                'ç”µå•† æ­¥éª¤2 è¿›è´§ä»·æ ¼å¤„ç†åæ•°æ®.xlsx': step2_price,
                'ç”µå•† æ­¥éª¤3 åˆ©æ¶¦ä¿®æ­£åæ•°æ®.xlsx': step3_profit,
                'ç”µå•† æ­¥éª¤4 å¼‚å¸¸ä¿®æ­£åŠåˆ©æ¶¦é‡ç®—åæ•°æ®.xlsx': step4_abnormal,
                'ç”µå•† æ­¥éª¤5 MinMaxæ ‡å‡†åŒ–åæ•°æ®.xlsx': step5_minmax,
                'ç”µå•† æ­¥éª¤5 ZScoreæ ‡å‡†åŒ–åæ•°æ®.xlsx': step5_zscore
            }

            # æ•´ç†è¿›åº¦æ—¥å¿—
            progress_log = [
                f"æ­¥éª¤1ï¼šå®Œæˆç¼ºå¤±å€¼ç»Ÿè®¡ï¼Œå…±{len(step1_missing)}ä¸ªå­—æ®µ",
                f"æ­¥éª¤2ï¼šå®Œæˆè¿›è´§ä»·æ ¼å¤„ç†",
                f"æ­¥éª¤3ï¼šå®Œæˆåˆ©æ¶¦ä¿®æ­£",
                f"æ­¥éª¤4ï¼šå®Œæˆå¼‚å¸¸å€¼ä¿®æ­£",
                f"æ­¥éª¤5ï¼šå®Œæˆæ ‡å‡†åŒ–å¤„ç†ï¼Œç”ŸæˆMinMaxå’ŒZScoreä¸¤ç§æ ‡å‡†åŒ–ç»“æœ"
            ]

            return result_files, progress_log, final_data, encoders, self.column_types

        except Exception as e:
            return None, [f"é¢„å¤„ç†é”™è¯¯: {str(e)}"], None, None, None
# ============================================================================
# å¢å¼ºç‰ˆä»»åŠ¡2ï¼šå¤šç»´é”€å”®ç‰¹å¾åˆ†æç±»ï¼ˆæŒ‰è®ºæ–‡è¦æ±‚é‡æ„ï¼‰
# ============================================================================
# ============================================================================
# å¢å¼ºç‰ˆä»»åŠ¡2ï¼šå¤šç»´é”€å”®ç‰¹å¾åˆ†æç±»ï¼ˆæŒ‰è®ºæ–‡è¦æ±‚é‡æ„ï¼‰- ä¿®å¤çƒ­åŠ›å›¾é”™è¯¯
# ============================================================================
class EnhancedTask2Analyzer:
    def __init__(self, df, column_types):
        self.df = df.copy()
        self.column_types = column_types
        self.results = {}

    def create_heatmaps(self):
        """åˆ›å»ºçƒ­åŠ›å›¾ - ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜"""
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
            plt.rcParams['axes.unicode_minus'] = False

            figs = {}

            # 1. å•†å“å“ç±»ä¸çœä»½äº¤å‰çƒ­åŠ›å›¾
            if all(col in self.df.columns for col in ['åŒºåŸŸ', 'å•†å“å“ç±»', 'åˆ©æ¶¦']):
                # æå–çœä»½
                if self.df['åŒºåŸŸ'].str.contains('-').any():
                    self.df['çœä»½'] = self.df['åŒºåŸŸ'].apply(lambda x: x.split('-')[1] if '-' in str(x) else x)
                else:
                    self.df['çœä»½'] = self.df['åŒºåŸŸ']

                # ç¡®ä¿åˆ©æ¶¦åˆ—æ˜¯æ•°å€¼ç±»å‹
                self.df['åˆ©æ¶¦'] = pd.to_numeric(self.df['åˆ©æ¶¦'], errors='coerce')

                # è¿‡æ»¤æ‰æ— æ•ˆæ•°æ®
                heatmap_data = self.df[['å•†å“å“ç±»', 'çœä»½', 'åˆ©æ¶¦']].dropna()

                if len(heatmap_data) > 0:
                    plt.figure(figsize=(12, 8))
                    category_province_pivot = heatmap_data.pivot_table(
                        index='å•†å“å“ç±»',
                        columns='çœä»½',
                        values='åˆ©æ¶¦',
                        aggfunc='sum',
                        fill_value=0
                    )

                    # ç¡®ä¿æ•°æ®æ˜¯æ•°å€¼ç±»å‹
                    category_province_pivot = category_province_pivot.astype(float)

                    # é™åˆ¶è¡Œåˆ—æ•°é‡ï¼Œé¿å…çƒ­åŠ›å›¾å½¢çŠ¶è¿‡å¤§
                    if len(category_province_pivot) > 20:
                        category_province_pivot = category_province_pivot.head(20)
                    if len(category_province_pivot.columns) > 15:
                        category_province_pivot = category_province_pivot[category_province_pivot.columns[:15]]

                    sns.heatmap(category_province_pivot, cmap='Blues', annot=False, fmt='.0f')
                    plt.title('å•†å“å“ç±»å’Œçœä»½äº¤å‰çš„åˆ©æ¶¦çƒ­åŠ›å›¾')
                    plt.xlabel('çœä»½')
                    plt.xticks(rotation=45)
                    plt.ylabel('å•†å“å“ç±»')
                    plt.tight_layout()
                    figs['category_province_profit'] = plt.gcf()
                    plt.close()
                else:
                    st.warning("å•†å“å“ç±»-çœä»½çƒ­åŠ›å›¾ï¼šæ— æœ‰æ•ˆæ•°æ®")

            # 2. çœä»½ä¸æ—¥æœŸäº¤å‰çƒ­åŠ›å›¾
            if all(col in self.df.columns for col in ['æ—¥æœŸ', 'çœä»½', 'åˆ©æ¶¦']):
                # ç¡®ä¿æ•°æ®æ˜¯æ•°å€¼ç±»å‹
                self.df['åˆ©æ¶¦'] = pd.to_numeric(self.df['åˆ©æ¶¦'], errors='coerce')
                self.df['æ—¥æœŸ'] = pd.to_numeric(self.df['æ—¥æœŸ'], errors='coerce')

                # è¿‡æ»¤æ‰æ— æ•ˆæ•°æ®
                heatmap_data = self.df[['æ—¥æœŸ', 'çœä»½', 'åˆ©æ¶¦']].dropna()

                if len(heatmap_data) > 0:
                    plt.figure(figsize=(15, 8))
                    province_date_pivot = heatmap_data.pivot_table(
                        index='çœä»½',
                        columns='æ—¥æœŸ',
                        values='åˆ©æ¶¦',
                        aggfunc='sum',
                        fill_value=0
                    )

                    # ç¡®ä¿æ•°æ®æ˜¯æ•°å€¼ç±»å‹
                    province_date_pivot = province_date_pivot.astype(float)

                    # é™åˆ¶è¡Œåˆ—æ•°é‡
                    if len(province_date_pivot) > 15:
                        province_date_pivot = province_date_pivot.head(15)
                    if len(province_date_pivot.columns) > 20:
                        province_date_pivot = province_date_pivot[province_date_pivot.columns[:20]]

                    sns.heatmap(province_date_pivot, cmap='Blues', annot=False, fmt='.0f')
                    plt.title('çœä»½å’Œæ—¥æœŸäº¤å‰çš„åˆ©æ¶¦çƒ­åŠ›å›¾')
                    plt.xlabel('æ—¥æœŸ')
                    plt.xticks(rotation=90)
                    plt.ylabel('çœä»½')
                    plt.tight_layout()
                    figs['province_date_profit'] = plt.gcf()
                    plt.close()
                else:
                    st.warning("çœä»½-æ—¥æœŸçƒ­åŠ›å›¾ï¼šæ— æœ‰æ•ˆæ•°æ®")

            self.results['heatmaps'] = figs
            return len(figs) > 0

        except Exception as e:
            st.error(f"çƒ­åŠ›å›¾ç”Ÿæˆé”™è¯¯: {str(e)}")
            import traceback
            st.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False

    def perform_clustering_analysis(self):
        """æ‰§è¡Œèšç±»åˆ†æ - ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜"""
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
            plt.rcParams['axes.unicode_minus'] = False

            # é€‰æ‹©æ•°å€¼å‹åˆ—è¿›è¡Œèšç±»
            numeric_cols = ['å®¢æˆ·å¹´é¾„', 'è¿›è´§ä»·æ ¼', 'å®é™…å”®ä»·', 'é”€å”®æ•°', 'é”€å”®é¢', 'åˆ©æ¶¦']
            existing_numeric_cols = [col for col in numeric_cols if col in self.df.columns]

            if len(existing_numeric_cols) < 2:
                st.warning(f"å¯ç”¨äºèšç±»çš„æ•°å€¼å‹åˆ—ä¸è¶³ï¼Œä»…æ‰¾åˆ°: {existing_numeric_cols}")
                return False

            # æå–æ•°å€¼å‹æ•°æ®å¹¶å¤„ç†ç¼ºå¤±å€¼
            df_numeric = self.df[existing_numeric_cols].copy()

            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
            for col in existing_numeric_cols:
                df_numeric[col] = pd.to_numeric(df_numeric[col], errors='coerce')

            df_numeric = df_numeric.fillna(0)

            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if df_numeric.isnull().any().any() or (df_numeric == 0).all().any():
                st.warning("èšç±»æ•°æ®åŒ…å«æ— æ•ˆå€¼ï¼Œè·³è¿‡èšç±»åˆ†æ")
                return False

            # ç¡®å®šæœ€ä½³èšç±»æ•°k
            sse = []
            silhouette_scores = []
            k_range = range(2, min(11, len(df_numeric) // 2))  # é¿å…kå€¼è¿‡å¤§

            for k in k_range:
                try:
                    kmeans = KMeans(n_clusters=k, random_state=2024, n_init='auto')
                    kmeans.fit(df_numeric)
                    sse.append(kmeans.inertia_)
                    labels = kmeans.labels_
                    if len(set(labels)) > 1:  # ç¡®ä¿æœ‰å¤šä¸ªèšç±»
                        score = silhouette_score(df_numeric, labels)
                        silhouette_scores.append(score)
                    else:
                        silhouette_scores.append(0)
                except Exception as e:
                    st.warning(f"èšç±»æ•°k={k}æ—¶å‡ºé”™: {e}")
                    sse.append(0)
                    silhouette_scores.append(0)

            # ç»˜åˆ¶è¯„ä¼°å›¾è¡¨
            if len(sse) > 0 and len(silhouette_scores) > 0:
                fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
                ax1.plot(k_range, sse, 'bx-')
                ax1.set_xlabel('èšç±»æ•°é‡k')
                ax1.set_ylabel('SSEï¼ˆè¯¯å·®å¹³æ–¹å’Œï¼‰')
                ax1.set_title('æ‰‹è‚˜æ³•ç¡®å®šæœ€ä½³kå€¼')
                ax2.plot(k_range, silhouette_scores, 'rx-')
                ax2.set_xlabel('èšç±»æ•°é‡k')
                ax2.set_ylabel('è½®å»“ç³»æ•°')
                ax2.set_title('è½®å»“ç³»æ•°ç¡®å®šæœ€ä½³kå€¼')
                plt.tight_layout()

                self.results['cluster_evaluation_plot'] = fig1

                # é€‰æ‹©æœ€ä½³kå€¼
                if max(silhouette_scores) > 0:
                    best_k_index = silhouette_scores.index(max(silhouette_scores))
                    best_k = k_range[best_k_index]

                    # ä½¿ç”¨æœ€ä½³kå€¼æ‰§è¡Œæœ€ç»ˆèšç±»
                    final_kmeans = KMeans(n_clusters=best_k, random_state=2024, n_init='auto')
                    cluster_labels = final_kmeans.fit_predict(df_numeric)

                    # ä¿å­˜èšç±»ç»“æœ
                    df_clustered = self.df.copy()
                    df_clustered['èšç±»æ ‡ç­¾'] = cluster_labels
                    cluster_analysis = df_clustered.groupby('èšç±»æ ‡ç­¾')[existing_numeric_cols].mean().round(2)

                    self.results['clustered_data'] = df_clustered
                    self.results['cluster_analysis'] = cluster_analysis
                    self.results['best_k'] = best_k

                    return True
                else:
                    st.warning("æ— æ³•ç¡®å®šæœ‰æ•ˆçš„æœ€ä½³kå€¼ï¼Œè·³è¿‡èšç±»")
                    return False
            else:
                st.warning("èšç±»è¯„ä¼°æ•°æ®ä¸è¶³ï¼Œè·³è¿‡èšç±»åˆ†æ")
                return False

        except Exception as e:
            st.error(f"èšç±»åˆ†æé”™è¯¯: {str(e)}")
            import traceback
            st.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False

    def generate_city_distribution_data(self):
        """ç”ŸæˆåŸå¸‚åˆ†å¸ƒæ•°æ®ï¼ˆå¯¹åº”è®ºæ–‡å›¾4ï¼‰"""
        if 'åŒºåŸŸ' not in self.df.columns:
            return None

        # æå–åŸå¸‚ä¿¡æ¯
        if self.df['åŒºåŸŸ'].str.contains('-').any():
            self.df['åŸå¸‚'] = self.df['åŒºåŸŸ'].apply(lambda x: x.split('-')[1] if '-' in str(x) else x)
        else:
            self.df['åŸå¸‚'] = self.df['åŒºåŸŸ']

        # åŸå¸‚ç”¨æˆ·æ•°ç»Ÿè®¡
        city_stats = self.df['åŸå¸‚'].value_counts().reset_index()
        city_stats.columns = ['åŸå¸‚', 'ç”¨æˆ·æ•°']
        city_stats = city_stats.head(15)  # Top 15åŸå¸‚

        return city_stats

    def generate_province_distribution_data(self):
        """ç”Ÿæˆçœä»½åˆ†å¸ƒæ•°æ®ï¼ˆå¯¹åº”è®ºæ–‡å›¾5ï¼‰"""
        if 'åŒºåŸŸ' not in self.df.columns:
            return None

        # æå–çœä»½ä¿¡æ¯
        if 'çœä»½' not in self.df.columns:
            if self.df['åŒºåŸŸ'].str.contains('-').any():
                self.df['çœä»½'] = self.df['åŒºåŸŸ'].apply(lambda x: x.split('-')[0] if '-' in str(x) else x)
            else:
                self.df['çœä»½'] = self.df['åŒºåŸŸ']

        province_stats = self.df['çœä»½'].value_counts().reset_index()
        province_stats.columns = ['çœä»½', 'ç”¨æˆ·æ•°']

        return province_stats

    def generate_city_tier_data(self):
        """ç”ŸæˆåŸå¸‚åˆ†çº§æ•°æ®ï¼ˆå¯¹åº”è®ºæ–‡å›¾6ï¼‰"""
        if 'åŸå¸‚' not in self.df.columns:
            return None

        # åŸå¸‚åˆ†çº§å®šä¹‰ï¼ˆæ ¹æ®è®ºæ–‡ï¼‰
        tier_1 = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³']
        tier_2 = ['æ˜†æ˜', 'ç¦å·', 'å¦é—¨', 'æ— é”¡', 'å“ˆå°”æ»¨', 'é•¿æ˜¥', 'å®æ³¢', 'æµå—', 'å¤§è¿', 'éƒ‘å·',
                  'é•¿æ²™', 'æˆéƒ½', 'æ­å·', 'å—äº¬', 'æ­¦æ±‰', 'è¥¿å®‰', 'è‹å·', 'å¤©æ´¥', 'é’å²›', 'æ²ˆé˜³',
                  'ä¸œè', 'ä½›å±±', 'åˆè‚¥', 'çŸ³å®¶åº„', 'å—å®', 'å¸¸å·', 'çƒŸå°', 'å”å±±', 'å¾å·', 'æ¸©å·']
        tier_3 = ['å…°å·', 'æµ·å£', 'ä¹Œé²æœ¨é½', 'è´µé˜³', 'é“¶å·', 'è¥¿å®', 'å‘¼å’Œæµ©ç‰¹', 'æ‹‰è¨', 'ä¿å®š',
                  'æƒ å·', 'ç æµ·', 'ä¸­å±±', 'æ±Ÿé—¨', 'è‚‡åº†', 'æ¸…è¿œ', 'éŸ¶å…³', 'æ¹›æ±Ÿ', 'èŒ‚å', 'é˜³æ±Ÿ',
                  'äº‘æµ®', 'æ±•å¤´', 'æ½®å·', 'æ­é˜³', 'æ±•å°¾', 'æ¢…å·', 'æ²³æº']

        def assign_city_tier(city):
            if pd.isna(city):
                return 'å…¶ä»–åŸå¸‚'
            city_str = str(city)
            if city_str in tier_1:
                return 'ä¸€çº¿åŸå¸‚'
            elif city_str in tier_2:
                return 'äºŒçº¿åŸå¸‚'
            elif city_str in tier_3:
                return 'ä¸‰çº¿åŸå¸‚'
            else:
                return 'å…¶ä»–åŸå¸‚'

        self.df['åŸå¸‚ç­‰çº§'] = self.df['åŸå¸‚'].apply(assign_city_tier)
        tier_stats = self.df['åŸå¸‚ç­‰çº§'].value_counts().reset_index()
        tier_stats.columns = ['åŸå¸‚ç­‰çº§', 'ç”¨æˆ·æ•°']
        tier_stats['å æ¯”'] = (tier_stats['ç”¨æˆ·æ•°'] / len(self.df) * 100).round(2)

        return tier_stats

    def generate_region_tier_data(self):
        """ç”ŸæˆåŒºåŸŸåˆ†çº§æ•°æ®ï¼ˆå¯¹åº”è®ºæ–‡å›¾7ï¼‰"""
        if 'çœä»½' not in self.df.columns:
            return None

        # åŒºåŸŸå®šä¹‰
        region_mapping = {
            'åå—': ['å¹¿ä¸œ', 'å¹¿è¥¿', 'æµ·å—', 'ç¦å»º'],
            'åä¸œ': ['ä¸Šæµ·', 'æ±Ÿè‹', 'æµ™æ±Ÿ', 'å®‰å¾½', 'æ±Ÿè¥¿', 'å±±ä¸œ'],
            'ååŒ—': ['åŒ—äº¬', 'å¤©æ´¥', 'æ²³åŒ—', 'å±±è¥¿', 'å†…è’™å¤'],
            'ä¸œåŒ—': ['è¾½å®', 'å‰æ—', 'é»‘é¾™æ±Ÿ'],
            'è¥¿å—': ['é‡åº†', 'å››å·', 'è´µå·', 'äº‘å—', 'è¥¿è—'],
            'è¥¿åŒ—': ['é™•è¥¿', 'ç”˜è‚ƒ', 'é’æµ·', 'å®å¤', 'æ–°ç–†'],
            'åä¸­': ['æ²³å—', 'æ¹–åŒ—', 'æ¹–å—']
        }

        def assign_region(province):
            if pd.isna(province):
                return 'å…¶ä»–'
            province_str = str(province)
            for region, provinces in region_mapping.items():
                if province_str in provinces:
                    return region
            return 'å…¶ä»–'

        self.df['å¤§åŒº'] = self.df['çœä»½'].apply(assign_region)
        region_stats = self.df['å¤§åŒº'].value_counts().reset_index()
        region_stats.columns = ['å¤§åŒº', 'ç”¨æˆ·æ•°']
        region_stats['å æ¯”'] = (region_stats['ç”¨æˆ·æ•°'] / len(self.df) * 100).round(2)

        return region_stats

    def generate_gender_category_analysis(self):
        """ç”Ÿæˆæ€§åˆ«-å“ç±»åˆ†ææ•°æ®ï¼ˆå¯¹åº”è®ºæ–‡å›¾8ï¼‰"""
        if not all(col in self.df.columns for col in ['å®¢æˆ·æ€§åˆ«', 'å•†å“å“ç±»']):
            return None

        gender_category_stats = self.df.groupby(['å•†å“å“ç±»', 'å®¢æˆ·æ€§åˆ«']).size().reset_index()
        gender_category_stats.columns = ['å•†å“å“ç±»', 'å®¢æˆ·æ€§åˆ«', 'è®¢å•äººæ•°']

        return gender_category_stats

    def generate_age_gender_analysis(self):
        """ç”Ÿæˆå¹´é¾„-æ€§åˆ«åˆ†ææ•°æ®ï¼ˆå¯¹åº”è®ºæ–‡å›¾9ï¼‰"""
        if 'å®¢æˆ·å¹´é¾„' not in self.df.columns or 'å®¢æˆ·æ€§åˆ«' not in self.df.columns:
            return None

        # ç¡®ä¿å¹´é¾„æ˜¯æ•°å€¼ç±»å‹
        self.df['å®¢æˆ·å¹´é¾„'] = pd.to_numeric(self.df['å®¢æˆ·å¹´é¾„'], errors='coerce')

        # å¹´é¾„åˆ†æ®µ
        def assign_age_group(age):
            if pd.isna(age):
                return 'æœªçŸ¥'
            try:
                age = int(age)
                if age < 25:
                    return '20-24å²'
                elif age < 30:
                    return '25-29å²'
                elif age < 35:
                    return '30-34å²'
                elif age < 40:
                    return '35-39å²'
                elif age < 45:
                    return '40-44å²'
                elif age < 50:
                    return '45-49å²'
                elif age < 55:
                    return '50-54å²'
                elif age < 60:
                    return '55-59å²'
                else:
                    return '60å²ä»¥ä¸Š'
            except:
                return 'æœªçŸ¥'

        self.df['å¹´é¾„æ®µ'] = self.df['å®¢æˆ·å¹´é¾„'].apply(assign_age_group)
        age_gender_stats = self.df.groupby(['å¹´é¾„æ®µ', 'å®¢æˆ·æ€§åˆ«']).size().reset_index()
        age_gender_stats.columns = ['å¹´é¾„æ®µ', 'å®¢æˆ·æ€§åˆ«', 'è®¢å•äººæ•°']

        return age_gender_stats

    def generate_time_series_analysis(self):
        """ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†ææ•°æ®ï¼ˆå¯¹åº”è®ºæ–‡å›¾10ï¼‰"""
        date_col = next((col for col in self.column_types['identifier'] if 'æ—¥æœŸ' in col), None)
        if not date_col:
            return None

        # ç¡®ä¿æ—¥æœŸæ˜¯æ•°å€¼ç±»å‹
        self.df[date_col] = pd.to_numeric(self.df[date_col], errors='coerce')

        time_stats = self.df.groupby(date_col).size().reset_index()
        time_stats.columns = ['æ—¥æœŸ', 'è®¢å•äººæ•°æ€»å’Œ']

        return time_stats

    def generate_correlation_analysis(self):
        """ç”Ÿæˆç›¸å…³æ€§åˆ†ææ•°æ®ï¼ˆå¯¹åº”è®ºæ–‡å›¾13ï¼‰"""
        numeric_cols = self.column_types['numeric']
        if len(numeric_cols) < 2:
            return None

        # ç¡®ä¿æ‰€æœ‰æ•°å€¼åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
        correlation_data = self.df[numeric_cols].copy()
        for col in numeric_cols:
            correlation_data[col] = pd.to_numeric(correlation_data[col], errors='coerce')

        correlation_data = correlation_data.dropna()

        if len(correlation_data) < 2:
            return None

        correlation_matrix = correlation_data.corr().round(4)

        return correlation_matrix

    def generate_all_analysis_data(self):
        """ç”Ÿæˆæ‰€æœ‰åˆ†æç»´åº¦çš„æ•°æ®"""
        analysis_results = {}

        # åœ°ç†åˆ†å¸ƒåˆ†æ
        analysis_results['city_distribution'] = self.generate_city_distribution_data()
        analysis_results['province_distribution'] = self.generate_province_distribution_data()
        analysis_results['city_tier_analysis'] = self.generate_city_tier_data()
        analysis_results['region_tier_analysis'] = self.generate_region_tier_data()

        # å®¢æˆ·ç”»åƒåˆ†æ
        analysis_results['gender_category_analysis'] = self.generate_gender_category_analysis()
        analysis_results['age_gender_analysis'] = self.generate_age_gender_analysis()

        # æ—¶é—´åºåˆ—åˆ†æ
        analysis_results['time_series_analysis'] = self.generate_time_series_analysis()

        # ç»Ÿè®¡å…³ç³»åˆ†æ
        analysis_results['correlation_analysis'] = self.generate_correlation_analysis()

        # ä¿ç•™åŸæœ‰çš„çƒ­åŠ›å›¾å’Œèšç±»åˆ†æ
        analysis_results.update(self.results)

        return analysis_results

def show_python_visualizations(analyzer):
    """æ˜¾ç¤ºPythonåŸç”Ÿå¯è§†åŒ–"""
    st.subheader("ğŸ“Š Pythonå¯è§†åŒ–å±•ç¤º")

    # åŸæœ‰çš„çƒ­åŠ›å›¾å’Œèšç±»åˆ†æå±•ç¤º
    if 'heatmaps' in analyzer.results and len(analyzer.results['heatmaps']) > 0:
        st.subheader("1. äº¤å‰ç»´åº¦çƒ­åŠ›å›¾åˆ†æ")
        for name, fig in analyzer.results['heatmaps'].items():
            st.pyplot(fig)

    if 'cluster_evaluation_plot' in analyzer.results:
        st.subheader("2. èšç±»åˆ†æç»“æœ")
        st.pyplot(analyzer.results['cluster_evaluation_plot'])

        if 'cluster_analysis' in analyzer.results:
            st.subheader("èšç±»ç‰¹å¾å¹³å‡å€¼å¯¹æ¯”")
            st.dataframe(analyzer.results['cluster_analysis'])


def show_data_export_interface(analysis_data):
    """æ˜¾ç¤ºæ•°æ®å¯¼å‡ºç•Œé¢ - ä½¿ç”¨Excelæ ¼å¼é¿å…ç¼–ç é—®é¢˜"""
    st.subheader("ğŸ“ è®ºæ–‡å›¾è¡¨æ•°æ®å¯¼å‡º")

    st.markdown("""
    ### å¯¼å‡ºè¯´æ˜
    ä»¥ä¸‹æ•°æ®å¯ç›´æ¥ç”¨äºåœ¨Excelã€Tableauã€Echartsç­‰å·¥å…·ä¸­åˆ¶ä½œè®ºæ–‡å›¾è¡¨ã€‚
    ä¸ºé¿å…ç¼–ç é—®é¢˜ï¼Œå·²æä¾›Excelæ ¼å¼ä¸‹è½½ã€‚
    """)

    def convert_to_excel(df, sheet_name="æ•°æ®"):
        """å°†DataFrameè½¬æ¢ä¸ºExcelæ ¼å¼"""
        import io
        output = io.BytesIO()

        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name=sheet_name, index=False)

        output.seek(0)
        return output.getvalue()

    # åœ°ç†åˆ†å¸ƒæ•°æ®å¯¼å‡º
    st.markdown("#### ğŸŒ åœ°ç†åˆ†å¸ƒåˆ†æ")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        if analysis_data.get('city_distribution') is not None:
            excel_data = convert_to_excel(analysis_data['city_distribution'], "åŸå¸‚åˆ†å¸ƒ")
            st.download_button(
                label="ä¸‹è½½åŸå¸‚åˆ†å¸ƒæ•°æ®",
                data=excel_data,
                file_name="åŸå¸‚åˆ†å¸ƒæ•°æ®_Top15åŸå¸‚.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    with col2:
        if analysis_data.get('province_distribution') is not None:
            excel_data = convert_to_excel(analysis_data['province_distribution'], "çœä»½åˆ†å¸ƒ")
            st.download_button(
                label="ä¸‹è½½çœä»½åˆ†å¸ƒæ•°æ®",
                data=excel_data,
                file_name="çœä»½åˆ†å¸ƒæ•°æ®.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    with col3:
        if analysis_data.get('city_tier_analysis') is not None:
            excel_data = convert_to_excel(analysis_data['city_tier_analysis'], "åŸå¸‚åˆ†çº§")
            st.download_button(
                label="ä¸‹è½½åŸå¸‚åˆ†çº§æ•°æ®",
                data=excel_data,
                file_name="åŸå¸‚åˆ†çº§ç¯çŠ¶å›¾æ•°æ®.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    with col4:
        if analysis_data.get('region_tier_analysis') is not None:
            excel_data = convert_to_excel(analysis_data['region_tier_analysis'], "åŒºåŸŸåˆ†çº§")
            st.download_button(
                label="ä¸‹è½½åŒºåŸŸåˆ†çº§æ•°æ®",
                data=excel_data,
                file_name="åŒºåŸŸåˆ†çº§ç¯çŠ¶å›¾æ•°æ®.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    # å®¢æˆ·ç”»åƒæ•°æ®å¯¼å‡º
    st.markdown("#### ğŸ‘¥ å®¢æˆ·ç”»åƒåˆ†æ")
    col1, col2 = st.columns(2)

    with col1:
        if analysis_data.get('gender_category_analysis') is not None:
            excel_data = convert_to_excel(analysis_data['gender_category_analysis'], "æ€§åˆ«å“ç±»")
            st.download_button(
                label="ä¸‹è½½æ€§åˆ«-å“ç±»æ•°æ®",
                data=excel_data,
                file_name="æ€§åˆ«å“ç±»äº¤å‰åˆ†ææ•°æ®.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    with col2:
        if analysis_data.get('age_gender_analysis') is not None:
            excel_data = convert_to_excel(analysis_data['age_gender_analysis'], "å¹´é¾„æ€§åˆ«")
            st.download_button(
                label="ä¸‹è½½å¹´é¾„-æ€§åˆ«æ•°æ®",
                data=excel_data,
                file_name="å¹´é¾„æ€§åˆ«åˆ†å¸ƒæ•°æ®.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    # æ—¶é—´åºåˆ—å’Œç›¸å…³æ€§åˆ†æ
    st.markdown("#### ğŸ“ˆ æ—¶é—´ä¸å…³ç³»åˆ†æ")
    col1, col2 = st.columns(2)

    with col1:
        if analysis_data.get('time_series_analysis') is not None:
            excel_data = convert_to_excel(analysis_data['time_series_analysis'], "æ—¶é—´åºåˆ—")
            st.download_button(
                label="ä¸‹è½½æ—¶é—´åºåˆ—æ•°æ®",
                data=excel_data,
                file_name="æ—¶é—´åºåˆ—è®¢å•æ•°æ®.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    with col2:
        if analysis_data.get('correlation_analysis') is not None:
            # ç›¸å…³æ€§çŸ©é˜µéœ€è¦ä¿ç•™ç´¢å¼•
            import io
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                analysis_data['correlation_analysis'].to_excel(writer, sheet_name="ç›¸å…³æ€§çŸ©é˜µ")
            excel_data = output.getvalue()
            st.download_button(
                label="ä¸‹è½½ç›¸å…³æ€§çŸ©é˜µ",
                data=excel_data,
                file_name="å˜é‡ç›¸å…³æ€§çŸ©é˜µ.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    # æ•°æ®é¢„è§ˆï¼ˆä¿æŒä¸å˜ï¼‰
    st.markdown("#### ğŸ‘€ æ•°æ®é¢„è§ˆ")
    available_datasets = [key for key in analysis_data.keys() if
                          analysis_data[key] is not None and hasattr(analysis_data[key], 'head')]
    if available_datasets:
        dataset_to_preview = st.selectbox(
            "é€‰æ‹©è¦é¢„è§ˆçš„æ•°æ®é›†:",
            available_datasets
        )

        if dataset_to_preview:
            st.dataframe(analysis_data[dataset_to_preview].head(10))

            # æ•°æ®ç»Ÿè®¡ä¿¡æ¯
            st.markdown("**æ•°æ®ç»Ÿè®¡:**")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("è¡Œæ•°", len(analysis_data[dataset_to_preview]))
            with col2:
                st.metric("åˆ—æ•°", len(analysis_data[dataset_to_preview].columns))
            with col3:
                st.metric("æ•°æ®ç±»å‹", str(analysis_data[dataset_to_preview].dtypes.unique()[0]))

    st.success("âœ… ç°åœ¨ä¸‹è½½çš„Excelæ–‡ä»¶åº”è¯¥ä¸ä¼šå‡ºç°ä¹±ç é—®é¢˜äº†ï¼")


# ============================================================================
# ä»»åŠ¡3ï¼šé”€å”®é¢„æµ‹ç±»ï¼ˆä¿®å¤ç‰ˆ - ä¸æºä»£ç ä¿æŒä¸€è‡´ï¼‰
# ============================================================================
class Task3Forecaster:
    def __init__(self, df, column_types):
        self.df = df.copy()
        self.column_types = column_types
        self.results = {}

    def prepare_time_series_data(self):
        """ä½¿ç”¨æºä»£ç çš„æ•°æ®å‡†å¤‡é€»è¾‘ - ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜"""
        try:
            # ä½¿ç”¨æºä»£ç çš„ç›´æ¥è½¬æ¢æ–¹å¼
            date_col = next((col for col in self.column_types['identifier'] if 'æ—¥æœŸ' in col), None)
            if not date_col:
                st.error("æœªè¯†åˆ«åˆ°æ—¥æœŸå­—æ®µï¼Œæ— æ³•æ„å»ºæ—¶é—´åºåˆ—")
                return False

            # æ”¹ä¸ºæºä»£ç çš„è½¬æ¢æ–¹å¼
            self.df[date_col] = self.df[date_col].astype(int)

            # ç¡®ä¿åˆ©æ¶¦åˆ—æ˜¯æ•°å€¼ç±»å‹ï¼ˆæºä»£ç æ–¹å¼ï¼‰
            self.df['åˆ©æ¶¦'] = pd.to_numeric(self.df['åˆ©æ¶¦'], errors='coerce')
            self.df = self.df.dropna(subset=['åˆ©æ¶¦'])

            # æŒ‰æ—¥èšåˆåˆ©æ¶¦æ•°æ®
            daily_profit = self.df.groupby(date_col)['åˆ©æ¶¦'].sum().reset_index()
            daily_profit = daily_profit.rename(columns={'åˆ©æ¶¦': 'æ¯æ—¥æ€»åˆ©æ¶¦'})

            # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            train = daily_profit[daily_profit[date_col] <= 24]
            test = daily_profit[daily_profit[date_col] > 24]

            if len(train) == 0 or len(test) == 0:
                st.error("æ•°æ®æ—¥æœŸèŒƒå›´ä¸è¶³ï¼Œæ— æ³•åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†")
                return False

            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®æ˜¯æ ‡å‡†çš„numpyæ•°ç»„ï¼Œä¸æ˜¯IntegerArray
            self.results['time_series_data'] = daily_profit
            self.results['train_data'] = train
            self.results['test_data'] = test
            self.results['date_col'] = date_col

            # è½¬æ¢ä¸ºæ ‡å‡†çš„numpyæ•°ç»„ï¼Œé¿å…IntegerArrayé—®é¢˜
            self.results['y_train'] = train['æ¯æ—¥æ€»åˆ©æ¶¦'].values.astype(float)
            self.results['y_test'] = test['æ¯æ—¥æ€»åˆ©æ¶¦'].values.astype(float)

            st.success(f"æ—¶é—´åºåˆ—å‡†å¤‡å®Œæˆï¼šè®­ç»ƒé›†{len(train)}å¤©ï¼Œæµ‹è¯•é›†{len(test)}å¤©")
            return True

        except Exception as e:
            st.error(f"æ—¶é—´åºåˆ—å‡†å¤‡é”™è¯¯: {str(e)}")
            return False

    def create_features(self, day_indices, residuals=None):
        """ä½¿ç”¨æºä»£ç çš„ç‰¹å¾å·¥ç¨‹é€»è¾‘"""
        features = []

        # é¢„è®¡ç®—æ¯ä¸ªæ—¥æœŸçš„ç»Ÿè®¡é‡ï¼ˆæºä»£ç é€»è¾‘ï¼‰
        daily_stats = self.df.groupby(self.results['date_col']).agg({
            'é”€å”®é¢': ['count', 'mean', 'sum'],
            'å®é™…å”®ä»·': 'mean',
            'è¿›è´§ä»·æ ¼': 'mean',
            'å®¢æˆ·æ€§åˆ«': lambda x: (x == 'å¥³').mean()
        }).round(4)

        daily_stats.columns = ['order_count', 'avg_sale', 'total_sale',
                               'avg_selling_price', 'avg_cost_price', 'female_ratio']

        # æºä»£ç çš„æ¯›åˆ©ç‡è®¡ç®—ï¼ˆä¸å¤„ç†é™¤0ï¼‰
        daily_stats['gross_profit_margin'] = (
                (daily_stats['avg_selling_price'] - daily_stats['avg_cost_price']) /
                daily_stats['avg_cost_price']
        ).fillna(0).round(4)

        # æºä»£ç çš„å•å®¢ä»·å€¼è®¡ç®—
        daily_stats['customer_value'] = (
                daily_stats['total_sale'] / daily_stats['order_count']
        ).fillna(0).round(2)

        # è®­ç»ƒé›†ç»Ÿè®¡é‡ï¼ˆç”¨äºå¡«å……ç¼ºå¤±å€¼ï¼‰- æºä»£ç é€»è¾‘
        train_days_data = self.df[self.df[self.results['date_col']] <= 24]
        train_stats = train_days_data.groupby(self.results['date_col']).agg({
            'é”€å”®é¢': ['count', 'mean', 'sum'],
            'å®¢æˆ·æ€§åˆ«': lambda x: (x == 'å¥³').mean()
        })
        train_stats.columns = ['order_count', 'avg_sale', 'total_sale', 'female_ratio']

        for day in day_indices:
            day_features = {}

            # 1. åŸºç¡€æ—¶é—´ç‰¹å¾ï¼ˆä¸æºä»£ç ä¸€è‡´ï¼‰
            day_features['day'] = int(day)
            day_features['day_of_week'] = (int(day) - 1) % 7
            day_features['day_of_month'] = int(day)
            day_features['is_weekend'] = 1 if day_features['day_of_week'] in [5, 6] else 0
            day_features['is_month_end'] = 1 if int(day) >= 28 else 0

            # 2. ä»é¢„è®¡ç®—çš„ç»Ÿè®¡é‡ä¸­è·å–ä¸šåŠ¡ç‰¹å¾ï¼ˆæºä»£ç é€»è¾‘ï¼‰
            if day in daily_stats.index:
                stats = daily_stats.loc[day]
                day_features.update({
                    'order_count': float(stats['order_count']),
                    'avg_sale_amount': float(stats['avg_sale']),
                    'total_sale': float(stats['total_sale']),
                    'gross_profit_margin': float(stats['gross_profit_margin']),
                    'customer_value': float(stats['customer_value']),
                    'female_ratio': float(stats['female_ratio'])
                })
            else:
                # ä½¿ç”¨æºä»£ç çš„ä¸­ä½æ•°å¡«å……é€»è¾‘
                day_features.update({
                    'order_count': float(train_stats['order_count'].median()),
                    'avg_sale_amount': float(train_stats['avg_sale'].median()),
                    'total_sale': float(train_stats['total_sale'].median()),
                    'gross_profit_margin': float(0.3),  # æºä»£ç çš„é»˜è®¤å€¼
                    'customer_value': float(
                        train_stats['total_sale'].median() / max(1, train_stats['order_count'].median())),
                    'female_ratio': float(train_stats['female_ratio'].median())
                })

            # 3. æ»åæ®‹å·®ç‰¹å¾ï¼ˆæºä»£ç é€»è¾‘ï¼‰
            if residuals is not None:
                for lag in [1, 2, 3]:
                    lag_day = int(day) - lag
                    lag_key = f'residual_lag_{lag}'
                    if lag_day > 0 and lag_day in residuals.index:
                        day_features[lag_key] = float(residuals[lag_day])
                    else:
                        day_features[lag_key] = float(residuals.median() if not residuals.empty else 0)

            features.append(day_features)

        return pd.DataFrame(features)

    def hybrid_forecast(self):
        """ä½¿ç”¨æºä»£ç çš„ARIMA-XGBoostæ··åˆé¢„æµ‹é€»è¾‘ - ä¿®å¤æ•°æ®ç±»å‹"""
        try:
            from statsmodels.tsa.arima.model import ARIMA
            from xgboost import XGBRegressor
            from sklearn.metrics import mean_absolute_percentage_error

            # è·å–æ•°æ®
            train = self.results['train_data']
            test = self.results['test_data']
            y_train = self.results['y_train']  # å·²ç»æ˜¯floatæ•°ç»„
            y_test = self.results['y_test']  # å·²ç»æ˜¯floatæ•°ç»„
            date_col = self.results['date_col']

            # 1. ARIMAå»ºæ¨¡ - ä½¿ç”¨æºä»£ç å‚æ•° (2,1,2)
            st.info("Step 1: ARIMAå»ºæ¨¡...")
            try:
                # ğŸ”¥ ç¡®ä¿y_trainæ˜¯æ ‡å‡†çš„numpy floatæ•°ç»„
                y_train_arima = y_train.astype(float)

                arima_model = ARIMA(y_train_arima, order=(2, 1, 2))  # æ”¹ä¸ºæºä»£ç å‚æ•°
                arima_fit = arima_model.fit()
                arima_train_pred = arima_fit.predict(start=0, end=len(y_train_arima) - 1)
                arima_test_pred = arima_fit.forecast(steps=len(y_test))
                st.success(f"ARIMAæ¨¡å‹è®­ç»ƒæˆåŠŸ (AIC: {arima_fit.aic:.2f})")
            except Exception as e:
                st.warning(f"ARIMAæ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œä½¿ç”¨å‡å€¼é¢„æµ‹: {e}")
                # ä½¿ç”¨numpyæ•°ç»„é¿å…æ•°æ®ç±»å‹é—®é¢˜
                arima_train_pred = np.full_like(y_train, float(np.mean(y_train)))
                arima_test_pred = np.full_like(y_test, float(np.mean(y_train)))
                arima_fit = None

            # 2. è®¡ç®—æ®‹å·® - ç¡®ä¿æ˜¯floatç±»å‹
            residuals_train = y_train.astype(float) - arima_train_pred.astype(float)
            residual_series = pd.Series(residuals_train, index=train[date_col].values)

            # 3. XGBoostå­¦ä¹ æ®‹å·® - ä½¿ç”¨æºä»£ç å‚æ•°
            st.info("Step 2: XGBoostå­¦ä¹ æ®‹å·®...")

            # åˆ›å»ºç‰¹å¾
            X_train = self.create_features(train[date_col].values, residual_series)
            X_train = X_train.fillna(0)

            # ç¡®ä¿ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹
            for col in X_train.columns:
                X_train[col] = pd.to_numeric(X_train[col], errors='coerce')
            X_train = X_train.fillna(0)

            # XGBoostæ¨¡å‹è®­ç»ƒ - æºä»£ç å‚æ•°
            xgb_model = XGBRegressor(
                max_depth=3,
                learning_rate=0.05,
                n_estimators=1000,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                random_state=42,
                eval_metric='mae'
            )
            xgb_model.fit(X_train, residuals_train)

            # æµ‹è¯•é›†ç‰¹å¾
            X_test = self.create_features(test[date_col].values)
            X_test = X_test.fillna(0)

            # ç¡®ä¿ç‰¹å¾åˆ—ä¸€è‡´
            for col in X_train.columns:
                if col not in X_test.columns:
                    X_test[col] = 0
            X_test = X_test[X_train.columns]

            # ç¡®ä¿æµ‹è¯•é›†ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹
            for col in X_test.columns:
                X_test[col] = pd.to_numeric(X_test[col], errors='coerce')
            X_test = X_test.fillna(0)

            # é¢„æµ‹æ®‹å·®
            xgb_residual_pred = xgb_model.predict(X_test)

            # 4. æœ€ç»ˆé¢„æµ‹
            final_pred = arima_test_pred.astype(float) + xgb_residual_pred.astype(float)
            mape = mean_absolute_percentage_error(y_test, final_pred) * 100

            # ä¿å­˜ç»“æœ
            self.results['arima_model'] = arima_fit
            self.results['xgb_model'] = xgb_model
            self.results['arima_test_pred'] = arima_test_pred.astype(float)
            self.results['xgb_residual_pred'] = xgb_residual_pred.astype(float)
            self.results['final_pred'] = final_pred.astype(float)
            self.results['mape'] = mape
            self.results['residuals_train'] = residuals_train.astype(float)
            self.results['feature_importance'] = pd.DataFrame({
                'feature': X_train.columns,
                'importance': xgb_model.feature_importances_
            }).sort_values('importance', ascending=False)

            # åˆ›å»ºè¯¦ç»†ç»“æœè¡¨
            results_df = pd.DataFrame({
                'æ—¥æœŸ': test[date_col].values,
                'å®é™…åˆ©æ¶¦': y_test,
                'ARIMAé¢„æµ‹': arima_test_pred.astype(float),
                'XGBoostæ®‹å·®é¢„æµ‹': xgb_residual_pred.astype(float),
                'æœ€ç»ˆé¢„æµ‹': final_pred.astype(float),
                'ç›¸å¯¹è¯¯å·®(%)': (np.abs(y_test - final_pred) / y_test * 100).astype(float)
            })
            self.results['detailed_results'] = results_df

            st.success(f"æ··åˆé¢„æµ‹å®Œæˆï¼æµ‹è¯•é›†MAPE: {mape:.2f}%")
            return True

        except Exception as e:
            st.error(f"æ··åˆé¢„æµ‹é”™è¯¯: {str(e)}")
            import traceback
            st.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False

    def generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ - ä¿æŒä¸å˜"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns

            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False

            figs = {}

            # 1. ä¸»é¢„æµ‹å¯¹æ¯”å›¾
            fig1, ax1 = plt.subplots(figsize=(12, 8))
            train = self.results['train_data']
            test = self.results['test_data']
            date_col = self.results['date_col']
            y_train = self.results['y_train']
            y_test = self.results['y_test']

            # ç»˜åˆ¶è®­ç»ƒé›†å®é™…å€¼
            ax1.plot(train[date_col], y_train / 10000, 'bo-', label='è®­ç»ƒé›†å®é™…å€¼',
                     alpha=0.7, markersize=6, linewidth=2)
            # ç»˜åˆ¶æµ‹è¯•é›†å®é™…å€¼
            ax1.plot(test[date_col], y_test / 10000, 'ro-', label='æµ‹è¯•é›†å®é™…å€¼',
                     alpha=0.7, markersize=8, linewidth=2)

            # ç»˜åˆ¶ARIMAè®­ç»ƒé›†æ‹Ÿåˆå€¼ï¼ˆæºä»£ç ä¸­çš„å›¾è¡¨ï¼‰
            try:
                arima_train_fit = self.results['arima_model'].predict(start=1, end=24)
                ax1.plot(train[date_col], arima_train_fit / 10000, 'c--',
                         label='ARIMAè®­ç»ƒé›†æ‹Ÿåˆ', alpha=0.8, linewidth=2)
            except:
                pass

            # ç»˜åˆ¶ARIMAæµ‹è¯•é›†é¢„æµ‹å€¼
            ax1.plot(test[date_col], self.results['arima_test_pred'] / 10000, 'm--',
                     label='ARIMAæµ‹è¯•é›†é¢„æµ‹', alpha=0.8, linewidth=2)
            # ç»˜åˆ¶æœ€ç»ˆç»„åˆé¢„æµ‹å€¼
            ax1.plot(test[date_col], self.results['final_pred'] / 10000, 'gs-',
                     label='ARIMA+XGBoostæœ€ç»ˆé¢„æµ‹', markersize=8, linewidth=2)

            ax1.set_xlabel('æ—¥æœŸ (11æœˆå¤©æ•°)', fontsize=12)
            ax1.set_ylabel('åˆ©æ¶¦ (ä¸‡å…ƒ)', fontsize=12)
            ax1.set_title('åˆ©æ¶¦é¢„æµ‹å¯¹æ¯”å›¾', fontsize=14, fontweight='bold')
            ax1.legend(fontsize=11)
            ax1.grid(True, alpha=0.3)
            ax1.axvline(x=24.5, color='gray', linestyle=':', alpha=0.7, linewidth=2)
            ax1.text(24.7, ax1.get_ylim()[1] * 0.9, 'æµ‹è¯•é›†å¼€å§‹', rotation=90, va='top', fontsize=10)
            figs['main_forecast'] = fig1

            # 2. è¯¯å·®åˆ†æå›¾
            fig2, ax2 = plt.subplots(figsize=(10, 6))
            relative_errors = self.results['detailed_results']['ç›¸å¯¹è¯¯å·®(%)']
            bars = ax2.bar(test[date_col], relative_errors, alpha=0.7, color='orange',
                           edgecolor='darkorange', linewidth=1)

            ax2.set_xlabel('æ—¥æœŸ (11æœˆå¤©æ•°)', fontsize=12)
            ax2.set_ylabel('ç›¸å¯¹è¯¯å·® (%)', fontsize=12)
            ax2.set_title(f'é¢„æµ‹è¯¯å·®åˆ†æ (MAPE = {self.results["mape"]:.2f}%)',
                          fontsize=14, fontweight='bold')
            ax2.grid(True, alpha=0.3, axis='y')

            # æ·»åŠ è¯¯å·®æ•°å€¼æ ‡ç­¾
            for date, error in zip(test[date_col], relative_errors):
                ax2.text(date, error + 1, f'{error:.1f}%', ha='center', va='bottom',
                         fontsize=10, fontweight='bold')
            figs['error_analysis'] = fig2

            # 3. æ®‹å·®åˆ†æå›¾
            fig3, ax3 = plt.subplots(figsize=(12, 6))

            if 'residuals_train' in self.results:
                residuals_train = self.results['residuals_train']

                # ç»˜åˆ¶æ®‹å·®
                train_dates = train[date_col].values
                ax3.plot(train_dates, residuals_train, 'o-', color='purple',
                         alpha=0.7, markersize=6, linewidth=2, label='æ¯æ—¥æ®‹å·®')

                # é›¶åŸºå‡†çº¿
                ax3.axhline(y=0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='é›¶åŸºå‡†çº¿')

                # å‡å€¼çº¿
                mean_residual = residuals_train.mean()
                ax3.axhline(y=mean_residual, color='blue', linestyle=':', linewidth=2, alpha=0.7,
                            label=f'å‡å€¼: {mean_residual:.2f}')

                ax3.set_xlabel('è®­ç»ƒé›†æ—¥æœŸ (11æœˆå¤©æ•°)', fontsize=12)
                ax3.set_ylabel('æ®‹å·®å€¼', fontsize=12)
                ax3.set_title('ARIMAæ¨¡å‹æ®‹å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
                ax3.legend(fontsize=11)
                ax3.grid(True, alpha=0.3)
                ax3.set_xticks(train_dates)

                # ç»Ÿè®¡ä¿¡æ¯æ¡†
                stats_text = (f'å‡å€¼: {residuals_train.mean():.2f}\n'
                              f'æ ‡å‡†å·®: {residuals_train.std():.2f}\n'
                              f'æœ€å¤§å€¼: {residuals_train.max():.2f}\n'
                              f'æœ€å°å€¼: {residuals_train.min():.2f}')

                ax3.text(0.02, 0.98, stats_text, transform=ax3.transAxes, fontsize=11,
                         verticalalignment='top', bbox=dict(boxstyle="round,pad=0.3",
                                                            facecolor="lightgray", alpha=0.7))

                figs['residual_analysis'] = fig3

            # 4. ç‰¹å¾é‡è¦æ€§å›¾
            fig4, ax4 = plt.subplots(figsize=(12, 8))

            if 'feature_importance' in self.results:
                feature_importance = self.results['feature_importance'].head(10)

                # ç‰¹å¾åç§°æ˜ å°„
                feature_names_map = {
                    'day': 'æ—¥æœŸ', 'day_of_week': 'æ˜ŸæœŸ', 'day_of_month': 'æœˆå†…å¤©æ•°',
                    'is_weekend': 'æ˜¯å¦å‘¨æœ«', 'is_month_end': 'æ˜¯å¦æœˆæœ«',
                    'order_count': 'è®¢å•æ•°', 'avg_sale_amount': 'å¹³å‡é”€å”®é¢',
                    'total_sale': 'æ€»é”€å”®é¢', 'gross_profit_margin': 'æ¯›åˆ©ç‡',
                    'customer_value': 'å•å®¢ä»·å€¼', 'female_ratio': 'å¥³æ€§æ¯”ä¾‹',
                    'residual_lag_1': 'æ®‹å·®æ»å1å¤©', 'residual_lag_2': 'æ®‹å·®æ»å2å¤©',
                    'residual_lag_3': 'æ®‹å·®æ»å3å¤©'
                }

                feature_importance['feature_cn'] = feature_importance['feature'].map(
                    lambda x: feature_names_map.get(x, x)
                )
                feature_importance = feature_importance.sort_values('importance', ascending=True)

                y_pos = np.arange(len(feature_importance))
                colors = plt.cm.viridis(np.linspace(0, 1, len(feature_importance)))

                ax4.barh(y_pos, feature_importance['importance'], color=colors,
                         alpha=0.8, edgecolor='black')
                ax4.set_yticks(y_pos)
                ax4.set_yticklabels(feature_importance['feature_cn'], fontsize=11)
                ax4.set_xlabel('ç‰¹å¾é‡è¦æ€§å¾—åˆ†', fontsize=12, fontweight='bold')
                ax4.set_title('XGBoostç‰¹å¾é‡è¦æ€§æ’å', fontsize=14, fontweight='bold')
                ax4.grid(True, alpha=0.3, axis='x')

                # æ·»åŠ é‡è¦æ€§æ•°å€¼æ ‡ç­¾
                for i, v in enumerate(feature_importance['importance']):
                    ax4.text(v + 0.005, i, f'{v:.3f}', va='center', fontsize=10, fontweight='bold')

                figs['feature_importance'] = fig4

            self.results['visualizations'] = figs
            return True

        except Exception as e:
            st.error(f"å¯è§†åŒ–ç”Ÿæˆé”™è¯¯: {str(e)}")
            return False

    def generate_all_results(self, forecast_days=14):
        """ç”Ÿæˆæ‰€æœ‰é¢„æµ‹ç»“æœ"""
        try:
            if not self.prepare_time_series_data():
                return None, ["æ—¶é—´åºåˆ—æ•°æ®å‡†å¤‡å¤±è´¥"]

            if not self.hybrid_forecast():
                return None, ["æ··åˆé¢„æµ‹æ¨¡å‹æ‰§è¡Œå¤±è´¥"]

            if not self.generate_visualizations():
                return None, ["å¯è§†åŒ–ç”Ÿæˆå¤±è´¥"]

            # æ•´ç†ç»“æœæ–‡ä»¶
            result_files = {
                '01_æ—¶é—´åºåˆ—å†å²æ•°æ®.xlsx': self.results['time_series_data'],
                '02_é”€å”®é¢„æµ‹ç»“æœ.xlsx': self.results['detailed_results'],
                '03_ç‰¹å¾é‡è¦æ€§åˆ†æ.xlsx': self.results['feature_importance']
            }

            # è¿›åº¦æ—¥å¿—
            progress_log = [
                f"æ—¶é—´åºåˆ—å‡†å¤‡å®Œæˆï¼šè®­ç»ƒé›†{len(self.results['train_data'])}å¤©ï¼Œæµ‹è¯•é›†{len(self.results['test_data'])}å¤©",
                f"ARIMA-XGBoostæ··åˆé¢„æµ‹å®Œæˆï¼šæµ‹è¯•é›†MAPE {self.results['mape']:.2f}%",
                f"ç‰¹å¾é‡è¦æ€§åˆ†æå®Œæˆï¼š{len(self.results['feature_importance'])}ä¸ªç‰¹å¾",
                f"å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆï¼š{len(self.results['visualizations'])}ä¸ªåˆ†æå›¾è¡¨"
            ]

            return result_files, progress_log

        except Exception as e:
            return None, [f"é¢„æµ‹é”™è¯¯: {str(e)}"]
# ============================================================================
# ä»»åŠ¡4ï¼šè¿è¥ç­–ç•¥ä¼˜åŒ–ç±»
# ============================================================================
class Task4Optimizer:
    def __init__(self, df, column_types):
        self.df = df.copy()
        self.column_types = column_types
        self.results = {}

    def abc_analysis(self):
        """ABCåˆ†ç±»åˆ†æ"""
        try:
            # 1. æŒ‰å•†å“å“ç±»ABCåˆ†ç±»
            if all(x in self.df.columns for x in ['å•†å“å“ç±»', 'é”€å”®é¢', 'åˆ©æ¶¦']):
                category_metrics = self.df.groupby('å•†å“å“ç±»').agg({
                    'é”€å”®é¢': 'sum',
                    'åˆ©æ¶¦': 'sum',
                    'é”€å”®æ•°': 'count'
                }).reset_index()
                category_metrics = category_metrics.sort_values('é”€å”®é¢', ascending=False)
                category_metrics['é”€å”®é¢ç´¯è®¡å æ¯”%'] = (
                        category_metrics['é”€å”®é¢'].cumsum() / category_metrics['é”€å”®é¢'].sum() * 100).round(2)
                category_metrics['åˆ©æ¶¦ç´¯è®¡å æ¯”%'] = (
                        category_metrics['åˆ©æ¶¦'].cumsum() / category_metrics['åˆ©æ¶¦'].sum() * 100).round(2)

                # ABCåˆ†ç±»è§„åˆ™
                def assign_abc(cumulative_percent):
                    if cumulative_percent <= 70:
                        return 'Aç±»ï¼ˆæ ¸å¿ƒï¼‰'
                    elif cumulative_percent <= 90:
                        return 'Bç±»ï¼ˆæ½œåŠ›ï¼‰'
                    else:
                        return 'Cç±»ï¼ˆé•¿å°¾ï¼‰'

                category_metrics['ABCåˆ†ç±»ï¼ˆæŒ‰é”€å”®é¢ï¼‰'] = category_metrics['é”€å”®é¢ç´¯è®¡å æ¯”%'].apply(assign_abc)
                category_metrics['ABCåˆ†ç±»ï¼ˆæŒ‰åˆ©æ¶¦ï¼‰'] = category_metrics['åˆ©æ¶¦ç´¯è®¡å æ¯”%'].apply(assign_abc)

                # 2. æŒ‰åŒºåŸŸABCåˆ†ç±»
                if 'åŒºåŸŸ' in self.df.columns:
                    region_metrics = self.df.groupby('åŒºåŸŸ').agg({
                        'é”€å”®é¢': 'sum',
                        'åˆ©æ¶¦': 'sum'
                    }).reset_index()
                    region_metrics = region_metrics.sort_values('é”€å”®é¢', ascending=False)
                    region_metrics['é”€å”®é¢ç´¯è®¡å æ¯”%'] = (
                            region_metrics['é”€å”®é¢'].cumsum() / region_metrics['é”€å”®é¢'].sum() * 100).round(2)
                    region_metrics['ABCåˆ†ç±»ï¼ˆæŒ‰é”€å”®é¢ï¼‰'] = region_metrics['é”€å”®é¢ç´¯è®¡å æ¯”%'].apply(assign_abc)

                    self.results['region_abc'] = region_metrics

                self.results['category_abc'] = category_metrics
                return True
            st.warning("ç¼ºå°‘ABCåˆ†ç±»æ‰€éœ€å­—æ®µï¼ˆå•†å“å“ç±»ã€é”€å”®é¢ã€åˆ©æ¶¦ï¼‰")
            return False
        except Exception as e:
            st.error(f"ABCåˆ†ç±»é”™è¯¯: {str(e)}")
            return False

    def price_sensitivity_analysis(self):
        """ä»·æ ¼æ•æ„Ÿåº¦åˆ†æ"""
        try:
            if not all(x in self.df.columns for x in ['å•†å“å“ç±»', 'å®é™…å”®ä»·', 'é”€å”®æ•°']):
                st.warning("ç¼ºå°‘ä»·æ ¼æ•æ„Ÿåº¦åˆ†ææ‰€éœ€å­—æ®µï¼ˆå•†å“å“ç±»ã€å®é™…å”®ä»·ã€é”€å”®æ•°ï¼‰")
                return False

            sensitivity_results = []
            # 1. æŒ‰å“ç±»åˆ†æä»·æ ¼æ•æ„Ÿåº¦
            for category in self.df['å•†å“å“ç±»'].unique():
                category_data = self.df[self.df['å•†å“å“ç±»'] == category].copy()
                if len(category_data) < 10:
                    st.info(f"å•†å“å“ç±»ã€{category}ã€‘æ ·æœ¬é‡ä¸è¶³10æ¡ï¼Œè·³è¿‡åˆ†æ")
                    continue

                # ç­‰é¢‘8åŒºé—´åˆ’åˆ†
                category_data['ä»·æ ¼åŒºé—´'] = pd.qcut(
                    category_data['å®é™…å”®ä»·'],
                    q=8,
                    labels=[f'åŒºé—´{i}' for i in range(1, 9)],
                    duplicates='drop'
                )
                price_sales = category_data.groupby('ä»·æ ¼åŒºé—´').agg({
                    'å®é™…å”®ä»·': 'mean',
                    'é”€å”®æ•°': 'sum'
                }).reset_index()

                # è®¡ç®—æ•æ„Ÿåº¦ç³»æ•°
                slope, intercept, r_value, p_value, std_err = linregress(
                    price_sales['å®é™…å”®ä»·'],
                    price_sales['é”€å”®æ•°']
                )
                sensitivity_coeff = slope / (price_sales['é”€å”®æ•°'].mean() / price_sales['å®é™…å”®ä»·'].mean())

                # æ•æ„Ÿåº¦ç­‰çº§åˆ¤å®š
                if sensitivity_coeff < -0.3:
                    level = 'é«˜æ•æ„Ÿåº¦ï¼ˆä»·æ ¼ä¸»å¯¼ï¼‰'
                elif sensitivity_coeff < -0.1:
                    level = 'ä¸­æ•æ„Ÿåº¦ï¼ˆä»·æ ¼+å“è´¨ï¼‰'
                else:
                    level = 'ä½æ•æ„Ÿåº¦ï¼ˆå“è´¨ä¸»å¯¼ï¼‰'

                sensitivity_results.append({
                    'åˆ†æç»´åº¦': 'å•†å“å“ç±»',
                    'ç»´åº¦å€¼': category,
                    'ä»·æ ¼å¼¹æ€§ç³»æ•°': sensitivity_coeff.round(4),
                    'RÂ²ï¼ˆæ‹Ÿåˆä¼˜åº¦ï¼‰': round(r_value ** 2, 4),
                    'æ•æ„Ÿåº¦ç­‰çº§': level,
                    'æ ·æœ¬é‡': len(category_data)
                })

            # 2. æŒ‰äººç¾¤åˆ†æä»·æ ¼æ•æ„Ÿåº¦
            if 'å®¢æˆ·æ€§åˆ«' in self.df.columns:
                st.info("å¼€å§‹æŒ‰å®¢æˆ·æ€§åˆ«åˆ†æä»·æ ¼æ•æ„Ÿåº¦")
                for gender in self.df['å®¢æˆ·æ€§åˆ«'].unique():
                    gender_data = self.df[self.df['å®¢æˆ·æ€§åˆ«'] == gender].copy()
                    if len(gender_data) < 20:
                        st.info(f"å®¢æˆ·æ€§åˆ«ã€{gender}ã€‘æ ·æœ¬é‡ä¸è¶³20æ¡ï¼Œè·³è¿‡åˆ†æ")
                        continue

                    gender_data['ä»·æ ¼åŒºé—´'] = pd.qcut(
                        gender_data['å®é™…å”®ä»·'],
                        q=8,
                        labels=[f'åŒºé—´{i}' for i in range(1, 9)],
                        duplicates='drop'
                    )
                    price_sales = gender_data.groupby('ä»·æ ¼åŒºé—´').agg({
                        'å®é™…å”®ä»·': 'mean',
                        'é”€å”®æ•°': 'sum'
                    }).reset_index()

                    slope, intercept, r_value, p_value, std_err = linregress(
                        price_sales['å®é™…å”®ä»·'],
                        price_sales['é”€å”®æ•°']
                    )
                    sensitivity_coeff = slope / (price_sales['é”€å”®æ•°'].mean() / price_sales['å®é™…å”®ä»·'].mean())

                    level = 'é«˜æ•æ„Ÿåº¦' if sensitivity_coeff < -0.3 else 'ä¸­æ•æ„Ÿåº¦' if sensitivity_coeff < -0.1 else 'ä½æ•æ„Ÿåº¦'
                    sensitivity_results.append({
                        'åˆ†æç»´åº¦': 'å®¢æˆ·æ€§åˆ«',
                        'ç»´åº¦å€¼': gender,
                        'ä»·æ ¼å¼¹æ€§ç³»æ•°': sensitivity_coeff.round(4),
                        'RÂ²ï¼ˆæ‹Ÿåˆä¼˜åº¦ï¼‰': round(r_value ** 2, 4),
                        'æ•æ„Ÿåº¦ç­‰çº§': level,
                        'æ ·æœ¬é‡': len(gender_data)
                    })

            sensitivity_df = pd.DataFrame(sensitivity_results)
            self.results['price_sensitivity'] = sensitivity_df
            st.success("ä»·æ ¼æ•æ„Ÿåº¦åˆ†æå®Œæˆ")

            # å¯è§†åŒ–ï¼šé«˜æ•æ„Ÿåº¦å“ç±»TOP5
            high_sensitivity = sensitivity_df[sensitivity_df['åˆ†æç»´åº¦'] == 'å•†å“å“ç±»'].nsmallest(5, 'ä»·æ ¼å¼¹æ€§ç³»æ•°')
            if len(high_sensitivity) > 0:
                fig, ax = plt.subplots(figsize=(10, 6))
                sns.barplot(x='ç»´åº¦å€¼', y='ä»·æ ¼å¼¹æ€§ç³»æ•°', data=high_sensitivity, palette='Reds')
                ax.set_title('å•†å“å“ç±»ä»·æ ¼æ•æ„Ÿåº¦TOP5ï¼ˆå¼¹æ€§ç³»æ•°è¶Šå°è¶Šæ•æ„Ÿï¼‰', fontsize=14)
                ax.set_xlabel('å•†å“å“ç±»', fontsize=12)
                ax.set_ylabel('ä»·æ ¼å¼¹æ€§ç³»æ•°', fontsize=12)
                ax.axhline(y=-0.3, color='red', linestyle='--', alpha=0.7, label='é«˜æ•æ„Ÿåº¦é˜ˆå€¼ï¼ˆ-0.3ï¼‰')
                ax.legend()
                plt.xticks(rotation=45)
                self.results['sensitivity_plot'] = fig
                st.pyplot(fig)

            return True
        except Exception as e:
            st.error(f"ä»·æ ¼æ•æ„Ÿåº¦åˆ†æé”™è¯¯: {str(e)}")
            return False

    def generate_operation_strategy(self):
        """ç”Ÿæˆè¿è¥ç­–ç•¥"""
        try:
            strategies = []
            # 1. å•†å“å“ç±»ç­–ç•¥
            if 'category_abc' in self.results and 'price_sensitivity' in self.results:
                category_abc = self.results['category_abc']
                price_sensitivity = self.results['price_sensitivity'][
                    self.results['price_sensitivity']['åˆ†æç»´åº¦'] == 'å•†å“å“ç±»']

                for _, abc_row in category_abc.iterrows():
                    category = abc_row['å•†å“å“ç±»']
                    abc_sales = abc_row['ABCåˆ†ç±»ï¼ˆæŒ‰é”€å”®é¢ï¼‰']
                    sens_row = price_sensitivity[price_sensitivity['ç»´åº¦å€¼'] == category]
                    if len(sens_row) == 0:
                        continue
                    sens_level = sens_row['æ•æ„Ÿåº¦ç­‰çº§'].iloc[0]

                    if abc_sales == 'Aç±»ï¼ˆæ ¸å¿ƒï¼‰':
                        if sens_level == 'é«˜æ•æ„Ÿåº¦ï¼ˆä»·æ ¼ä¸»å¯¼ï¼‰':
                            strategy = "ä¿é”€é‡ï¼šæ—¥å¸¸å®šä»·ç»´æŒå“ç±»å‡ä»·-5%ï¼Œå¤§ä¿ƒæœŸé—´'æ»¡å‡+èµ å“'"
                            inventory = "é«˜å®‰å…¨åº“å­˜ï¼ˆæœˆé”€é‡1.5å€ï¼‰ï¼Œæå‰30å¤©å¤‡è´§"
                        else:
                            strategy = "æåˆ©æ¶¦ï¼šé«˜ç«¯æ¬¾æº¢ä»·10%-15%ï¼Œå¸¸è§„æ¬¾ç»´æŒå‡ä»·ï¼Œéå¤§ä¿ƒä¸é™ä»·"
                            inventory = "ä¸­ç­‰åº“å­˜ï¼ˆæœˆé”€é‡1.2å€ï¼‰ï¼Œå»ºç«‹åŒºåŸŸå…±äº«åº“å­˜æ± "
                    elif abc_sales == 'Bç±»ï¼ˆæ½œåŠ›ï¼‰':
                        strategy = "ä¿ƒè½¬åŒ–ï¼šç»„åˆä¿ƒé”€ï¼Œæ–°ç”¨æˆ·é¦–å•æŠ˜æ‰£5%-8%ï¼Œæå‡å“ç±»æ¸—é€ç‡"
                        inventory = "åŠ¨æ€åº“å­˜ï¼ˆå‚è€ƒé¢„æµ‹é”€é‡1.1å€ï¼‰ï¼Œæ¯æœˆè°ƒæ•´ä¸€æ¬¡"
                    else:
                        strategy = "æ¸…åº“å­˜ï¼šæ†ç»‘é”€å”®ï¼Œæˆ–é™æ—¶æŠ˜æ‰£30%-50%ï¼Œå‡å°‘èµ„é‡‘å ç”¨"
                        inventory = "ä½åº“å­˜ï¼ˆæœˆé”€é‡0.8å€ï¼‰ï¼Œæ»é”€è¶…60å¤©ç›´æ¥ä¸‹æ¶"

                    strategies.append({
                        'ç­–ç•¥ç»´åº¦': 'å•†å“å“ç±»',
                        'ç»´åº¦å€¼': category,
                        'ABCåˆ†ç±»': abc_sales,
                        'ä»·æ ¼æ•æ„Ÿåº¦': sens_level,
                        'å®šä»·ç­–ç•¥': strategy,
                        'åº“å­˜ç­–ç•¥': inventory,
                        'ä¼˜å…ˆçº§': 'é«˜' if abc_sales == 'Aç±»ï¼ˆæ ¸å¿ƒï¼‰' else 'ä¸­' if abc_sales == 'Bç±»ï¼ˆæ½œåŠ›ï¼‰' else 'ä½'
                    })

            # 2. åŒºåŸŸç­–ç•¥
            if 'region_abc' in self.results:
                for _, region_row in self.results['region_abc'].iterrows():
                    region = region_row['åŒºåŸŸ']
                    abc_sales = region_row['ABCåˆ†ç±»ï¼ˆæŒ‰é”€å”®é¢ï¼‰']

                    if abc_sales == 'Aç±»ï¼ˆæ ¸å¿ƒï¼‰':
                        strategy = "é‡ç‚¹æŠ•å…¥ï¼šå¢åŠ åŒºåŸŸä¸“å±ä¿ƒé”€ï¼Œä¼˜åŒ–ç‰©æµæ—¶æ•ˆï¼Œæå‡ç”¨æˆ·ç•™å­˜"
                        resource = "ä¼˜å…ˆé…ç½®ä»“å‚¨èµ„æºï¼Œå¢åŠ å®¢æœå›¢é˜Ÿ"
                    elif abc_sales == 'Bç±»ï¼ˆæ½œåŠ›ï¼‰':
                        strategy = "æ¸—é€æ‹“å±•ï¼šä¸åŒºåŸŸKOLåˆä½œæ¨å¹¿ï¼Œå¼€è®¾çº¿ä¸‹ä½“éªŒç‚¹"
                        resource = "é€‚åº¦æŠ•å…¥å¹¿å‘Šé¢„ç®—ï¼Œæµ‹è¯•ç”¨æˆ·åå¥½"
                    else:
                        strategy = "ä½æˆæœ¬è¦†ç›–ï¼šé€šè¿‡ç¤¾åŒºå›¢è´­ã€ä¸‹æ²‰æ¸ é“è§¦è¾¾"
                        resource = "æ§åˆ¶æˆæœ¬ï¼Œå¤ç”¨æ ¸å¿ƒåŒºåŸŸèµ„æº"

                    strategies.append({
                        'ç­–ç•¥ç»´åº¦': 'åŒºåŸŸ',
                        'ç»´åº¦å€¼': region,
                        'ABCåˆ†ç±»': abc_sales,
                        'è¿è¥ç­–ç•¥': strategy,
                        'èµ„æºé…ç½®': resource,
                        'ä¼˜å…ˆçº§': 'é«˜' if abc_sales == 'Aç±»ï¼ˆæ ¸å¿ƒï¼‰' else 'ä¸­' if abc_sales == 'Bç±»ï¼ˆæ½œåŠ›ï¼‰' else 'ä½'
                    })

            strategy_df = pd.DataFrame(strategies)
            self.results['operation_strategy'] = strategy_df
            return True
        except Exception as e:
            st.error(f"ç­–ç•¥ç”Ÿæˆé”™è¯¯: {str(e)}")
            return False

    def generate_all_results(self):
        """ç”Ÿæˆæ‰€æœ‰ä¼˜åŒ–ç»“æœ"""
        try:
            abc_success = self.abc_analysis()
            sensitivity_success = self.price_sensitivity_analysis()
            strategy_success = self.generate_operation_strategy() if (abc_success and sensitivity_success) else False

            # æ•´ç†ç»“æœæ–‡ä»¶
            result_files = {}
            progress_log = []

            if abc_success:
                result_files['01_ABCåˆ†ç±»ç»“æœï¼ˆå•†å“å“ç±»+åŒºåŸŸï¼‰.xlsx'] = pd.ExcelWriter(io.BytesIO())
                with result_files['01_ABCåˆ†ç±»ç»“æœï¼ˆå•†å“å“ç±»+åŒºåŸŸï¼‰.xlsx'] as writer:
                    self.results['category_abc'].to_excel(writer, sheet_name='å•†å“å“ç±»ABC', index=False)
                    if 'region_abc' in self.results:
                        self.results['region_abc'].to_excel(writer, sheet_name='åŒºåŸŸABC', index=False)
                result_files['01_ABCåˆ†ç±»ç»“æœï¼ˆå•†å“å“ç±»+åŒºåŸŸï¼‰.xlsx'] = result_files[
                    '01_ABCåˆ†ç±»ç»“æœï¼ˆå•†å“å“ç±»+åŒºåŸŸï¼‰.xlsx'].book
                progress_log.append("ABCåˆ†ç±»å®Œæˆï¼šå•†å“å“ç±»æŒ‰é”€å”®é¢/åˆ©æ¶¦åˆ†ç±»ï¼ŒåŒºåŸŸæŒ‰é”€å”®é¢åˆ†ç±»")

            if sensitivity_success:
                result_files['02_ä»·æ ¼æ•æ„Ÿåº¦åˆ†æç»“æœï¼ˆå“ç±»+äººç¾¤ï¼‰.xlsx'] = self.results['price_sensitivity']
                progress_log.append(
                    f"ä»·æ ¼æ•æ„Ÿåº¦åˆ†æå®Œæˆï¼šè¦†ç›–{len(self.results['price_sensitivity'])}ä¸ªç»´åº¦å€¼")

            if strategy_success:
                result_files['03_è¿è¥ç­–ç•¥æ¨è.xlsx'] = self.results['operation_strategy']
                progress_log.append(
                    f"è¿è¥ç­–ç•¥ç”Ÿæˆå®Œæˆï¼š{len(self.results['operation_strategy'])}æ¡ç­–ç•¥")

            return result_files, progress_log
        except Exception as e:
            return None, [f"ä¼˜åŒ–åˆ†æé”™è¯¯: {str(e)}"]


# ============================================================================
# é¡µé¢å‡½æ•°
# ============================================================================
def show_project_overview():
    """é¡¹ç›®æ¦‚è§ˆé¡µé¢"""
    st.header("ğŸ¯ é¡¹ç›®æ¦‚è§ˆ")

    st.markdown(
        '<div class="fix-note"><strong>ç³»ç»ŸåŠŸèƒ½ï¼š</strong><br>1. æ”¯æŒå¯¼å…¥ä»»æ„ç”µå•†Excelæ•°æ®ï¼ŒæŒ‰è®ºæ–‡æ ‡å‡†æµç¨‹å¤„ç†<br>2. è‡ªåŠ¨ç”Ÿæˆ6ä¸ªæ ‡å‡†åŒ–è¾“å‡ºæ–‡ä»¶<br>3. æä¾›å®Œæ•´çš„æ•°æ®åˆ†æã€å»ºæ¨¡å’Œå¯è§†åŒ–åŠŸèƒ½</div>',
        unsafe_allow_html=True)

    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
    ### ç³»ç»ŸåŠŸèƒ½æ¦‚è¿°
    å®Œæ•´çš„ç”µå•†é”€å”®åˆ†ææµç¨‹ï¼š
    - **æ•°æ®é¢„å¤„ç†**: æŒ‰è®ºæ–‡è¦æ±‚ç”Ÿæˆ6ä¸ªæ ‡å‡†åŒ–è¾“å‡ºæ–‡ä»¶
    - **å¤šç»´ç‰¹å¾åˆ†æ**: å“ç±»Ã—åŒºåŸŸÃ—åˆ©æ¶¦çƒ­åŠ›å›¾ã€å®¢æˆ·-å•†å“èšç±»
    - **é”€å”®é¢„æµ‹**: ARIMA+XGBoostæ··åˆæ¨¡å‹é¢„æµ‹æœªæ¥é”€å”®è¶‹åŠ¿
    - **è¿è¥ä¼˜åŒ–**: ABCåˆ†ç±»ã€ä»·æ ¼æ•æ„Ÿåº¦åˆ†æã€å¯è½åœ°çš„è¿è¥ç­–ç•¥
    """)

    with col2:
        st.metric("æ ‡å‡†è¾“å‡ºæ–‡ä»¶", "6ä¸ª")
        st.metric("åˆ†æä»»åŠ¡", "4ä¸ª")
        st.metric("æ”¯æŒæ•°æ®æ ¼å¼", "Excel/CSV")

    # ä»»åŠ¡çŠ¶æ€æ¦‚è§ˆ
    st.subheader("ä»»åŠ¡å®ŒæˆçŠ¶æ€")
    tasks = [
        ("æ•°æ®é¢„å¤„ç†", st.session_state.task1_completed),
        ("å¤šç»´åˆ†æ", st.session_state.task2_completed),
        ("é”€å”®é¢„æµ‹", st.session_state.task3_completed),
        ("è¿è¥ä¼˜åŒ–", st.session_state.task4_completed)
    ]

    for task_name, completed in tasks:
        status = "âœ… å·²å®Œæˆ" if completed else "â³ å¾…å®Œæˆ"
        st.write(f"- {task_name}: {status}")


def task1_data_preprocessing():
    """ä»»åŠ¡1ï¼šæ•°æ®é¢„å¤„ç†é¡µé¢ï¼ˆæŒ‰è®ºæ–‡è¦æ±‚ç”Ÿæˆæ ‡å‡†åŒ–è¾“å‡ºæ–‡ä»¶ï¼‰"""
    st.header("ğŸ“ ä»»åŠ¡1: æ•°æ®é¢„å¤„ç†")

    st.markdown(
        '<div class="fix-note"><strong>è®ºæ–‡è¦æ±‚è¾“å‡ºæ–‡ä»¶ï¼š</strong><br>1. ç”µå•† æ­¥éª¤1 ç¼ºå¤±å€¼ç»Ÿè®¡ç»“æœ.xlsx<br>2. ç”µå•† æ­¥éª¤2 è¿›è´§ä»·æ ¼å¤„ç†åæ•°æ®.xlsx<br>3. ç”µå•† æ­¥éª¤3 åˆ©æ¶¦ä¿®æ­£åæ•°æ®.xlsx<br>4. ç”µå•† æ­¥éª¤4 å¼‚å¸¸ä¿®æ­£åŠåˆ©æ¶¦é‡ç®—åæ•°æ®.xlsx<br>5. ç”µå•† æ­¥éª¤5 MinMaxæ ‡å‡†åŒ–åæ•°æ®.xlsx<br>6. ç”µå•† æ­¥éª¤5 ZScoreæ ‡å‡†åŒ–åæ•°æ®.xlsx</div>',
        unsafe_allow_html=True)

    # æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
    uploaded_file = st.file_uploader("ä¸Šä¼ åŸå§‹æ•°æ®è¡¨ï¼ˆæ”¯æŒExcelæˆ–CSVæ ¼å¼ï¼‰", type=["xlsx", "csv"])

    if uploaded_file is not None:
        # è¯»å–æ–‡ä»¶
        try:
            if uploaded_file.name.endswith('.xlsx'):
                df = pd.read_excel(uploaded_file)
            else:
                df = pd.read_csv(uploaded_file)

            # ä¿å­˜åŸå§‹æ•°æ®
            st.session_state.raw_data = df
            st.session_state.current_file = uploaded_file.name

            # æ•°æ®æ¸…æ´—ï¼šè‡ªåŠ¨å¤„ç†æ•°å€¼åˆ—ä¸­çš„éæ•°å€¼å­—ç¬¦
            df_clean = clean_numeric_columns(df)

            st.success(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼å…± {len(df)} æ¡è®°å½•ï¼Œ{len(df.columns)} ä¸ªå­—æ®µ")

            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆå’Œæ¸…æ´—å‰åå¯¹æ¯”
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("åŸå§‹æ•°æ®é¢„è§ˆï¼ˆå‰5è¡Œï¼‰")
                st.dataframe(df.head())

            with col2:
                st.subheader("æ¸…æ´—åæ•°æ®é¢„è§ˆï¼ˆå‰5è¡Œï¼‰")
                st.dataframe(df_clean.head())

            # æ˜¾ç¤ºæ•°æ®ç±»å‹ä¿¡æ¯
            st.subheader("æ•°æ®ç±»å‹ä¿¡æ¯")
            dtype_df = pd.DataFrame({
                'å­—æ®µå': df_clean.columns,
                'æ•°æ®ç±»å‹': df_clean.dtypes.astype(str)
            })
            st.dataframe(dtype_df)

            # æ‰§è¡Œé¢„å¤„ç†æŒ‰é’®
            if st.button("ğŸš€ å¼€å§‹æ•°æ®é¢„å¤„ç†ï¼ˆæŒ‰è®ºæ–‡æ­¥éª¤ï¼‰", type="primary"):
                with st.spinner("æ­£åœ¨æ‰§è¡Œæ•°æ®é¢„å¤„ç†...ï¼ˆæŒ‰è®ºæ–‡è¦æ±‚ç”Ÿæˆ6ä¸ªè¾“å‡ºæ–‡ä»¶ï¼‰"):
                    preprocessor = Task1Preprocessor(df_clean)
                    result_files, progress_log, final_data, encoders, column_types = preprocessor.generate_all_results()

                    if result_files:
                        # ä¿å­˜ç»“æœåˆ°session state
                        st.session_state.step1_missing_data = result_files['ç”µå•† æ­¥éª¤1 ç¼ºå¤±å€¼ç»Ÿè®¡ç»“æœ.xlsx']
                        st.session_state.step2_price_data = result_files['ç”µå•† æ­¥éª¤2 è¿›è´§ä»·æ ¼å¤„ç†åæ•°æ®.xlsx']
                        st.session_state.step3_profit_data = result_files['ç”µå•† æ­¥éª¤3 åˆ©æ¶¦ä¿®æ­£åæ•°æ®.xlsx']
                        st.session_state.step4_abnormal_data = result_files['ç”µå•† æ­¥éª¤4 å¼‚å¸¸ä¿®æ­£åŠåˆ©æ¶¦é‡ç®—åæ•°æ®.xlsx']
                        st.session_state.step5_minmax_data = result_files['ç”µå•† æ­¥éª¤5 MinMaxæ ‡å‡†åŒ–åæ•°æ®.xlsx']
                        st.session_state.step5_zscore_data = result_files['ç”µå•† æ­¥éª¤5 ZScoreæ ‡å‡†åŒ–åæ•°æ®.xlsx']
                        st.session_state.processed_data = final_data
                        st.session_state.category_encoder = encoders
                        st.session_state.column_types = column_types
                        st.session_state.task1_completed = True

                        st.success("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼å·²ç”Ÿæˆè®ºæ–‡è¦æ±‚çš„6ä¸ªè¾“å‡ºæ–‡ä»¶")

                        # 1. å±•ç¤ºé¢„å¤„ç†æ­¥éª¤ç»“æœé¢„è§ˆ
                        st.subheader("1. é¢„å¤„ç†æ­¥éª¤ç»“æœé¢„è§ˆ")

                        # æ­¥éª¤1ï¼šç¼ºå¤±å€¼ç»Ÿè®¡ç»“æœ
                        st.markdown("#### æ­¥éª¤1ï¼šç¼ºå¤±å€¼ç»Ÿè®¡ç»“æœ")
                        st.dataframe(st.session_state.step1_missing_data.head())

                        # æ­¥éª¤2ï¼šè¿›è´§ä»·æ ¼å¤„ç†åæ•°æ®
                        st.markdown("#### æ­¥éª¤2ï¼šè¿›è´§ä»·æ ¼å¤„ç†åæ•°æ®")
                        st.dataframe(st.session_state.step2_price_data[['å•†å“å“ç±»', 'è¿›è´§ä»·æ ¼']].head())

                        # æ­¥éª¤3ï¼šåˆ©æ¶¦ä¿®æ­£åæ•°æ®
                        st.markdown("#### æ­¥éª¤3ï¼šåˆ©æ¶¦ä¿®æ­£åæ•°æ®")
                        if 'åˆ©æ¶¦æ˜¯å¦æ­£ç¡®' in st.session_state.step3_profit_data.columns:
                            st.dataframe(
                                st.session_state.step3_profit_data[['å•†å“å“ç±»', 'åˆ©æ¶¦', 'åˆ©æ¶¦æ˜¯å¦æ­£ç¡®']].head())
                        else:
                            st.dataframe(st.session_state.step3_profit_data[['å•†å“å“ç±»', 'åˆ©æ¶¦']].head())

                        # 2. å±•ç¤ºé¢„å¤„ç†æ—¥å¿—
                        st.subheader("2. é¢„å¤„ç†æ‰§è¡Œæ—¥å¿—")
                        for log in progress_log:
                            st.write(f"â–ªï¸ {log}")

                        # 3. æä¾›ç»“æœæ–‡ä»¶ä¸‹è½½ï¼ˆæŒ‰è®ºæ–‡è¦æ±‚çš„æ–‡ä»¶åï¼‰
                        st.subheader("ğŸ“¥ è®ºæ–‡è¦æ±‚è¾“å‡ºæ–‡ä»¶ä¸‹è½½")
                        for filename, data in result_files.items():
                            if isinstance(data, pd.ExcelWriter):
                                excel_bytes = io.BytesIO()
                                data.save(excel_bytes)
                                excel_bytes.seek(0)
                                st.download_button(
                                    label=f"ä¸‹è½½ {filename}",
                                    data=excel_bytes,
                                    file_name=filename,
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                                )
                            elif isinstance(data, pd.DataFrame):
                                excel_bytes = io.BytesIO()
                                with pd.ExcelWriter(excel_bytes, engine='openpyxl') as writer:
                                    data.to_excel(writer, index=False)
                                st.download_button(
                                    label=f"ä¸‹è½½ {filename}",
                                    data=excel_bytes.getvalue(),
                                    file_name=filename,
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                                )

                        # æç¤ºä¸‹ä¸€æ­¥æ“ä½œ
                        st.info("é¢„å¤„ç†å®Œæˆï¼å¯ç»§ç»­è¿›è¡Œ å¤šç»´é”€å”®ç‰¹å¾åˆ†æï¼ˆä»»åŠ¡2ï¼‰")
                    else:
                        st.error("é¢„å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–æŸ¥çœ‹é”™è¯¯æ—¥å¿—")
                        for log in progress_log:
                            st.error(log)

        except Exception as e:
            st.error(f"æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}")
    else:
        st.info("è¯·ä¸Šä¼ åŸå§‹æ•°æ®æ–‡ä»¶å¼€å§‹é¢„å¤„ç†æµç¨‹ï¼ˆå»ºè®®åŒ…å«ï¼šå•†å“å“ç±»ã€åŒºåŸŸã€é”€å”®é¢ã€åˆ©æ¶¦ã€æ—¥æœŸç­‰å­—æ®µï¼‰")


def enhanced_task2_multidimensional_analysis():
    """å¢å¼ºç‰ˆå¤šç»´åˆ†æé¡µé¢"""
    st.header("ğŸ” ä»»åŠ¡2: å¤šç»´é”€å”®ç‰¹å¾åˆ†æ")

    if not st.session_state.task1_completed:
        st.warning("è¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†ï¼ˆä»»åŠ¡1ï¼‰")
        return

    df = st.session_state.processed_data
    column_types = st.session_state.column_types

    # åˆ†ææ¨¡å¼é€‰æ‹©
    analysis_mode = st.radio(
        "é€‰æ‹©åˆ†ææ¨¡å¼:",
        ["ğŸ“Š Pythonå¯è§†åŒ–å±•ç¤º", "ğŸ“ è®ºæ–‡å›¾è¡¨æ•°æ®å¯¼å‡º"],
        horizontal=True
    )

    if st.button("ğŸš€ æ‰§è¡Œå¤šç»´ç‰¹å¾åˆ†æ", type="primary"):
        with st.spinner("æ­£åœ¨æ‰§è¡Œå¤šç»´åˆ†æ..."):
            analyzer = EnhancedTask2Analyzer(df, column_types)

            # æ‰§è¡ŒåŸºç¡€åˆ†æï¼ˆçƒ­åŠ›å›¾å’Œèšç±»ï¼‰
            analyzer.create_heatmaps()
            analyzer.perform_clustering_analysis()

            # ç”Ÿæˆæ‰€æœ‰åˆ†ææ•°æ®
            all_analysis_data = analyzer.generate_all_analysis_data()

            st.session_state.task2_results = analyzer.results
            st.session_state.task2_analysis_data = all_analysis_data
            st.session_state.task2_completed = True

            st.success("âœ… å¤šç»´ç‰¹å¾åˆ†æå®Œæˆï¼")

            if analysis_mode == "ğŸ“Š Pythonå¯è§†åŒ–å±•ç¤º":
                show_python_visualizations(analyzer)
            else:
                show_data_export_interface(all_analysis_data)
    else:
        st.info("""
        **å¤šç»´ç‰¹å¾åˆ†æåŠŸèƒ½è¯´æ˜ï¼š**

        **ğŸ“Š Pythonå¯è§†åŒ–å±•ç¤ºæ¨¡å¼ï¼š**
        - äº¤å‰ç»´åº¦çƒ­åŠ›å›¾åˆ†æ
        - å®¢æˆ·-å•†å“èšç±»åˆ†æ
        - ç³»ç»Ÿå†…ç½®å¯è§†åŒ–å›¾è¡¨

        **ğŸ“ è®ºæ–‡å›¾è¡¨æ•°æ®å¯¼å‡ºæ¨¡å¼ï¼š**
        - åŸå¸‚åˆ†å¸ƒæ•°æ®
        - çœä»½åˆ†å¸ƒæ•°æ®
        - åŸå¸‚åˆ†çº§æ•°æ®
        - åŒºåŸŸåˆ†çº§æ•°æ®
        - æ€§åˆ«-å“ç±»æ•°æ®
        - å¹´é¾„-æ€§åˆ«æ•°æ®
        - æ—¶é—´åºåˆ—æ•°æ®
        - ç›¸å…³æ€§çŸ©é˜µæ•°æ®

        ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¼€å§‹åˆ†æï¼
        """)
def task3_sales_forecast():
    """ä»»åŠ¡3ï¼šé”€å”®é¢„æµ‹é¡µé¢"""
    st.header("ğŸ“ˆ ä»»åŠ¡3: é”€å”®é¢„æµ‹")

    if not st.session_state.get('task1_completed', False):
        st.warning("è¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†ï¼ˆä»»åŠ¡1ï¼‰")
        return

    # è·å–é¢„å¤„ç†åçš„æ•°æ®
    df = st.session_state.processed_data
    column_types = st.session_state.column_types

    # æ‰§è¡Œé¢„æµ‹
    if st.button("ğŸš€ æ‰§è¡ŒARIMA-XGBoostæ··åˆé¢„æµ‹", type="primary"):
        with st.spinner("é¢„æµ‹ä¸­...ï¼ˆä½¿ç”¨ARIMA(2,1,2)+XGBoostæ··åˆæ¨¡å‹ï¼‰"):
            forecaster = Task3Forecaster(df, column_types)
            result_files, progress_log = forecaster.generate_all_results()

            if result_files:
                st.session_state.task3_results = forecaster.results
                st.session_state.task3_completed = True
                st.success("âœ… é”€å”®é¢„æµ‹å®Œæˆï¼")

                # 1. å±•ç¤ºé¢„æµ‹ç»“æœå¯è§†åŒ–
                st.subheader("ğŸ“Š é¢„æµ‹ç»“æœå¯è§†åŒ–")

                if 'visualizations' in forecaster.results:
                    viz_results = forecaster.results['visualizations']

                    # åˆ©æ¶¦é¢„æµ‹å¯¹æ¯”å›¾ï¼ˆå¿…é¡»å±•ç¤ºï¼‰
                    st.markdown("#### 1. åˆ©æ¶¦é¢„æµ‹å¯¹æ¯”å›¾")
                    st.pyplot(viz_results['main_forecast'])
                    st.markdown("""
                    **å›¾è¡¨è¯´æ˜ï¼š**
                    - è“è‰²çº¿ï¼šè®­ç»ƒé›†å®é™…åˆ©æ¶¦å€¼
                    - çº¢è‰²çº¿ï¼šæµ‹è¯•é›†å®é™…åˆ©æ¶¦å€¼  
                    - ç²‰è‰²è™šçº¿ï¼šARIMAæ¨¡å‹é¢„æµ‹å€¼
                    - ç»¿è‰²çº¿ï¼šARIMA+XGBoostæœ€ç»ˆé¢„æµ‹å€¼
                    - ç°è‰²è™šçº¿ï¼šè®­ç»ƒé›†/æµ‹è¯•é›†åˆ†ç•Œçº¿
                    """)

                    # è¯¯å·®åˆ†æå›¾
                    st.markdown("#### 2. é¢„æµ‹è¯¯å·®åˆ†æå›¾")
                    st.pyplot(viz_results['error_analysis'])
                    st.markdown("""
                    **å›¾è¡¨è¯´æ˜ï¼š**
                    - æ˜¾ç¤ºæ¯æ—¥é¢„æµ‹çš„ç›¸å¯¹è¯¯å·®ç™¾åˆ†æ¯”
                    - MAPEï¼ˆå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼‰æ˜¯ä¸»è¦è¯„ä¼°æŒ‡æ ‡
                    - è¯¯å·®è¶Šå°ï¼Œæ¨¡å‹é¢„æµ‹ç²¾åº¦è¶Šé«˜
                    """)

                    # æ®‹å·®åˆ†æå›¾
                    st.markdown("#### 3. ARIMAæ¨¡å‹æ®‹å·®åˆ†æå›¾")
                    st.pyplot(viz_results['residual_analysis'])
                    st.markdown("""
                    **å›¾è¡¨è¯´æ˜ï¼š**
                    - æ˜¾ç¤ºARIMAæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„æ®‹å·®åˆ†å¸ƒ
                    - æ®‹å·®è¶Šæ¥è¿‘0ä¸”æ³¢åŠ¨è¶Šå°ï¼Œè¯´æ˜ARIMAæ¨¡å‹æ‹Ÿåˆè¶Šå¥½
                    - ä¸ºXGBoostæä¾›å­¦ä¹ ç›®æ ‡
                    """)

                    # ç‰¹å¾é‡è¦æ€§æ’åå›¾
                    st.markdown("#### 4. ç‰¹å¾é‡è¦æ€§æ’åå›¾")
                    st.pyplot(viz_results['feature_importance'])
                    st.markdown("""
                    **å›¾è¡¨è¯´æ˜ï¼š**
                    - æ˜¾ç¤ºXGBoostæ¨¡å‹ä¸­å„ç‰¹å¾çš„é‡è¦æ€§å¾—åˆ†
                    - é‡è¦æ€§è¶Šé«˜ï¼Œè¯¥ç‰¹å¾å¯¹æ®‹å·®é¢„æµ‹çš„è´¡çŒ®è¶Šå¤§
                    - å¸®åŠ©ç†è§£æ¨¡å‹å†³ç­–ä¾æ®
                    """)

                # 2. å±•ç¤ºé¢„æµ‹ç»“æœè¡¨æ ¼
                st.subheader("ğŸ“‹ é¢„æµ‹ç»“æœè¯¦æƒ…")
                if 'detailed_results' in forecaster.results:
                    forecast_df = forecaster.results['detailed_results']
                    st.dataframe(forecast_df.round(2))

                    # å…³é”®æŒ‡æ ‡æ€»ç»“
                    st.subheader("ğŸ¯ é¢„æµ‹å…³é”®æŒ‡æ ‡")
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        mape = forecaster.results.get('mape', 0)
                        st.metric("æµ‹è¯•é›†MAPE", f"{mape:.2f}%")

                    with col2:
                        best_error = forecast_df['ç›¸å¯¹è¯¯å·®(%)'].min()
                        best_day = forecast_df.loc[forecast_df['ç›¸å¯¹è¯¯å·®(%)'].idxmin(), 'æ—¥æœŸ']
                        st.metric("æœ€ä½³é¢„æµ‹ç²¾åº¦", f"{best_error:.1f}%", f"11æœˆ{int(best_day)}æ—¥")

                    with col3:
                        worst_error = forecast_df['ç›¸å¯¹è¯¯å·®(%)'].max()
                        worst_day = forecast_df.loc[forecast_df['ç›¸å¯¹è¯¯å·®(%)'].idxmax(), 'æ—¥æœŸ']
                        st.metric("æœ€å·®é¢„æµ‹ç²¾åº¦", f"{worst_error:.1f}%", f"11æœˆ{int(worst_day)}æ—¥")

                # 3. ç‰¹å¾é‡è¦æ€§åˆ†æ
                st.subheader("ğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ")
                if 'feature_importance' in forecaster.results:
                    feature_importance = forecaster.results['feature_importance']
                    st.dataframe(feature_importance.round(4))

                    st.info("""
                    **ç‰¹å¾é‡è¦æ€§è§£è¯»ï¼š**
                    - **é«˜é‡è¦æ€§ç‰¹å¾**ï¼šå¯¹æ¨¡å‹é¢„æµ‹å½±å“æœ€å¤§çš„å› ç´ 
                    - **ä¸­ç­‰é‡è¦æ€§ç‰¹å¾**ï¼šæœ‰ä¸€å®šé¢„æµ‹ä»·å€¼çš„è¾…åŠ©å› ç´   
                    - **ä½é‡è¦æ€§ç‰¹å¾**ï¼šå¯¹é¢„æµ‹ç»“æœå½±å“è¾ƒå°
                    """)

                # 4. è¿›åº¦æ—¥å¿—
                st.subheader("ğŸ“ é¢„æµ‹æ‰§è¡Œæ—¥å¿—")
                for log in progress_log:
                    st.write(f"â–ªï¸ {log}")

                # 5. æ–‡ä»¶ä¸‹è½½
                st.subheader("ğŸ“¥ é¢„æµ‹ç»“æœæ–‡ä»¶ä¸‹è½½")
                for filename, data in result_files.items():
                    if isinstance(data, pd.DataFrame):
                        excel_bytes = io.BytesIO()
                        with pd.ExcelWriter(excel_bytes, engine='openpyxl') as writer:
                            data.to_excel(writer, index=False)
                        st.download_button(
                            label=f"ä¸‹è½½ {filename}",
                            data=excel_bytes.getvalue(),
                            file_name=filename,
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )

                # æç¤ºä¸‹ä¸€æ­¥æ“ä½œ
                st.info("é”€å”®é¢„æµ‹å®Œæˆï¼å¯ç»§ç»­è¿›è¡Œ è¿è¥ç­–ç•¥ä¼˜åŒ–ï¼ˆä»»åŠ¡4ï¼‰")
            else:
                st.error("é¢„æµ‹å¤±è´¥ï¼Œè¯·æŸ¥çœ‹é”™è¯¯æ—¥å¿—")
                for log in progress_log:
                    st.error(log)
    else:
        st.info("""
        **ARIMA-XGBoostæ··åˆé¢„æµ‹æ¨¡å‹è¯´æ˜ï¼š**
        - ä½¿ç”¨ARIMA(2,1,2)æ¨¡å‹æ•æ‰æ—¶é—´åºåˆ—è¶‹åŠ¿
        - ä½¿ç”¨XGBoostæ¨¡å‹å­¦ä¹ ARIMAçš„æ®‹å·®æ¨¡å¼
        - æœ€ç»ˆé¢„æµ‹ = ARIMAé¢„æµ‹ + XGBoostæ®‹å·®é¢„æµ‹
        - æµ‹è¯•é›†ï¼š11æœˆ25-30æ—¥ï¼ˆå6å¤©æ•°æ®ï¼‰
        """)

def task4_operation_optimization():
    """ä»»åŠ¡4ï¼šè¿è¥ä¼˜åŒ–é¡µé¢"""
    st.header("ğŸ’¡ ä»»åŠ¡4: è¿è¥ç­–ç•¥ä¼˜åŒ–")

    if not st.session_state.task1_completed:
        st.warning("è¯·å…ˆå®Œæˆæ•°æ®é¢„å¤„ç†ï¼ˆä»»åŠ¡1ï¼‰")
        return

    # è·å–é¢„å¤„ç†åçš„æ•°æ®
    df = st.session_state.processed_data
    column_types = st.session_state.column_types

    # æ‰§è¡Œè¿è¥ä¼˜åŒ–åˆ†æ
    if st.button("ğŸš€ æ‰§è¡Œè¿è¥ä¼˜åŒ–åˆ†æ", type="primary"):
        with st.spinner("åˆ†æä¸­...ï¼ˆç”Ÿæˆå¯è½åœ°ç­–ç•¥ï¼‰"):
            optimizer = Task4Optimizer(df, column_types)
            result_files, progress_log = optimizer.generate_all_results()

            if result_files:
                st.session_state.task4_results = optimizer.results
                st.session_state.task4_completed = True
                st.success("âœ… è¿è¥ä¼˜åŒ–åˆ†æå®Œæˆï¼")

                # 1. å±•ç¤ºABCåˆ†ç±»ç»“æœ
                st.subheader("1. ABCåˆ†ç±»ç»“æœï¼ˆå•†å“å“ç±»+åŒºåŸŸï¼‰")
                if 'category_abc' in optimizer.results:
                    st.subheader("1.1 å•†å“å“ç±»ABCåˆ†ç±»ï¼ˆæŒ‰é”€å”®é¢/åˆ©æ¶¦ï¼‰")
                    category_abc = optimizer.results['category_abc']
                    st.dataframe(category_abc[['å•†å“å“ç±»', 'é”€å”®é¢', 'åˆ©æ¶¦', 'é”€å”®é¢ç´¯è®¡å æ¯”%',
                                               'ABCåˆ†ç±»ï¼ˆæŒ‰é”€å”®é¢ï¼‰']].round(2))

                if 'region_abc' in optimizer.results:
                    st.subheader("1.2 åŒºåŸŸABCåˆ†ç±»ï¼ˆæŒ‰é”€å”®é¢ï¼‰")
                    region_abc = optimizer.results['region_abc']
                    st.dataframe(region_abc[['åŒºåŸŸ', 'é”€å”®é¢', 'åˆ©æ¶¦', 'ABCåˆ†ç±»ï¼ˆæŒ‰é”€å”®é¢ï¼‰']].round(2))

                # 2. å±•ç¤ºä»·æ ¼æ•æ„Ÿåº¦åˆ†æç»“æœ
                st.subheader("2. ä»·æ ¼æ•æ„Ÿåº¦åˆ†æï¼ˆå“ç±»+äººç¾¤ï¼‰")
                if 'price_sensitivity' in optimizer.results:
                    sensitivity_df = optimizer.results['price_sensitivity']
                    # åˆ†ç»´åº¦å±•ç¤º
                    tab1, tab2 = st.tabs(["å•†å“å“ç±»æ•æ„Ÿåº¦", "å®¢æˆ·äººç¾¤æ•æ„Ÿåº¦"])

                    with tab1:
                        cat_sens = sensitivity_df[sensitivity_df['åˆ†æç»´åº¦'] == 'å•†å“å“ç±»']
                        cat_sens_sorted = cat_sens.sort_values('ä»·æ ¼å¼¹æ€§ç³»æ•°')
                        st.dataframe(cat_sens_sorted[['ç»´åº¦å€¼', 'ä»·æ ¼å¼¹æ€§ç³»æ•°', 'RÂ²ï¼ˆæ‹Ÿåˆä¼˜åº¦ï¼‰', 'æ•æ„Ÿåº¦ç­‰çº§',
                                                      'æ ·æœ¬é‡']].round(4))

                    with tab2:
                        people_sens = sensitivity_df[sensitivity_df['åˆ†æç»´åº¦'] == 'å®¢æˆ·æ€§åˆ«']
                        if len(people_sens) > 0:
                            st.dataframe(
                                people_sens[['ç»´åº¦å€¼', 'ä»·æ ¼å¼¹æ€§ç³»æ•°', 'æ•æ„Ÿåº¦ç­‰çº§', 'æ ·æœ¬é‡']].round(4))
                        else:
                            st.info("æš‚æ— å®¢æˆ·äººç¾¤æ•æ„Ÿåº¦æ•°æ®")

                # 3. å±•ç¤ºè¿è¥ç­–ç•¥æ¨è
                st.subheader("3. å¯è½åœ°è¿è¥ç­–ç•¥æ¨è")
                if 'operation_strategy' in optimizer.results:
                    strategy_df = optimizer.results['operation_strategy']
                    # æŒ‰ä¼˜å…ˆçº§ç­›é€‰å±•ç¤º
                    tab_high, tab_mid, tab_low = st.tabs(
                        ["é«˜ä¼˜å…ˆçº§ç­–ç•¥ï¼ˆAç±»æ ¸å¿ƒï¼‰", "ä¸­ä¼˜å…ˆçº§ç­–ç•¥ï¼ˆBç±»æ½œåŠ›ï¼‰", "ä½ä¼˜å…ˆçº§ç­–ç•¥ï¼ˆCç±»é•¿å°¾ï¼‰"])

                    with tab_high:
                        high_strategy = strategy_df[strategy_df['ä¼˜å…ˆçº§'] == 'é«˜']
                        if len(high_strategy) > 0:
                            st.dataframe(high_strategy)

                    with tab_mid:
                        mid_strategy = strategy_df[strategy_df['ä¼˜å…ˆçº§'] == 'ä¸­']
                        if len(mid_strategy) > 0:
                            st.dataframe(mid_strategy)

                    with tab_low:
                        low_strategy = strategy_df[strategy_df['ä¼˜å…ˆçº§'] == 'ä½']
                        if len(low_strategy) > 0:
                            st.dataframe(low_strategy)

                # 4. æ–‡ä»¶ä¸‹è½½
                st.subheader("ğŸ“¥ è¿è¥ä¼˜åŒ–ç»“æœæ–‡ä»¶ä¸‹è½½")
                for filename, data in result_files.items():
                    if isinstance(data, pd.ExcelWriter):
                        excel_bytes = io.BytesIO()
                        data.save(excel_bytes)
                        excel_bytes.seek(0)
                        st.download_button(
                            label=f"ä¸‹è½½ {filename}",
                            data=excel_bytes,
                            file_name=filename,
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                    elif isinstance(data, pd.DataFrame):
                        excel_bytes = io.BytesIO()
                        with pd.ExcelWriter(excel_bytes, engine='openpyxl') as writer:
                            data.to_excel(writer, index=False)
                        st.download_button(
                            label=f"ä¸‹è½½ {filename}",
                            data=excel_bytes.getvalue(),
                            file_name=filename,
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )

                # 5. åˆ†ææ—¥å¿—
                st.subheader("4. åˆ†ææ—¥å¿—")
                for log in progress_log:
                    st.write(f"â–ªï¸ {log}")


def show_system_status():
    """ç³»ç»ŸçŠ¶æ€é¡µé¢"""
    st.header("ğŸ”§ ç³»ç»ŸçŠ¶æ€")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ä»»åŠ¡å®ŒæˆçŠ¶æ€")
        tasks = [
            ("æ•°æ®é¢„å¤„ç†", st.session_state.task1_completed),
            ("å¤šç»´ç‰¹å¾åˆ†æ", st.session_state.task2_completed),
            ("é”€å”®é¢„æµ‹", st.session_state.task3_completed),
            ("è¿è¥ä¼˜åŒ–", st.session_state.task4_completed)
        ]
        for task_name, completed in tasks:
            status_class = "status-completed" if completed else "status-pending"
            icon = "âœ…" if completed else "â³"
            st.markdown(f'<div class="{status_class}">{icon} {task_name}</div>', unsafe_allow_html=True)

    with col2:
        st.subheader("æ•°æ®çŠ¶æ€")
        if st.session_state.raw_data is not None:
            df = st.session_state.raw_data
            total_records = len(df)
            total_cols = len(df.columns)
            numeric_cols = len(st.session_state.column_types['numeric']) if st.session_state.column_types else 0
            category_cols = len(st.session_state.column_types['nominal']) + len(
                st.session_state.column_types['ordinal']) if st.session_state.column_types else 0

            st.metric("æ€»è®°å½•æ•°", f"{total_records:,}æ¡")
            st.metric("æ€»å­—æ®µæ•°", f"{total_cols}ä¸ª")
            st.metric("æ•°å€¼å‹å­—æ®µ", f"{numeric_cols}ä¸ª")
            st.metric("åˆ†ç±»å‹å­—æ®µ", f"{category_cols}ä¸ª")
        else:
            st.info("æš‚æ— æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œ'æ•°æ®é¢„å¤„ç†'")

    # é‡ç½®åŠŸèƒ½
    if st.button("ğŸ”„ é‡ç½®ç³»ç»Ÿ", type="secondary"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        initialize_session_state()
        st.rerun()


# ============================================================================
# ä¸»åº”ç”¨å‡½æ•°
# ============================================================================
# ============================================================================
# ä¸»åº”ç”¨å‡½æ•° - ä¿®å¤è·¯ç”±é—®é¢˜
# ============================================================================
def main():
    """ä¸»åº”ç”¨å‡½æ•°"""
    # é¡µé¢é…ç½® - åœ¨è¿™é‡Œè®¾ç½®ä¸€æ¬¡
    st.set_page_config(
        page_title="ç”µå•†é”€å”®åˆ†æä¸ç­–ç•¥ä¼˜åŒ–ç³»ç»Ÿ",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.markdown('<div class="main-header">ğŸ“Š ç”µå•†é”€å”®åˆ†æä¸ç­–ç•¥ä¼˜åŒ–ç³»ç»Ÿ</div>',
                unsafe_allow_html=True)
    st.markdown(
        f"### å½“å‰æ–‡ä»¶ï¼š{st.session_state.get('current_file', 'æœªä¸Šä¼ æ–‡ä»¶')}")

    # ä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.title("å¯¼èˆªèœå•")
        selected_task = st.radio(
            "é€‰æ‹©åˆ†æä»»åŠ¡:",
            [
                "é¡¹ç›®æ¦‚è§ˆ",
                "æ•°æ®é¢„å¤„ç†",
                "å¤šç»´é”€å”®ç‰¹å¾åˆ†æ",
                "é”€å”®é¢„æµ‹åˆ†æ",
                "è¿è¥ç­–ç•¥ä¼˜åŒ–",
                "ç³»ç»ŸçŠ¶æ€"
            ]
        )

        # ä»»åŠ¡çŠ¶æ€æ¦‚è§ˆ
        st.markdown("---")
        st.subheader("ä»»åŠ¡å®ŒæˆçŠ¶æ€")
        tasks_status = [
            ("æ•°æ®é¢„å¤„ç†", st.session_state.task1_completed),
            ("å¤šç»´ç‰¹å¾åˆ†æ", st.session_state.task2_completed),
            ("é”€å”®é¢„æµ‹", st.session_state.task3_completed),
            ("è¿è¥ä¼˜åŒ–", st.session_state.task4_completed)
        ]
        for task_name, completed in tasks_status:
            status_class = "status-completed" if completed else "status-pending"
            icon = "âœ…" if completed else "â³"
            st.markdown(f'<div class="{status_class}">{icon} {task_name}</div>', unsafe_allow_html=True)

    # é¡µé¢è·¯ç”±
    if selected_task == "é¡¹ç›®æ¦‚è§ˆ":
        show_project_overview()
    elif selected_task == "æ•°æ®é¢„å¤„ç†":
        task1_data_preprocessing()
    elif selected_task == "å¤šç»´é”€å”®ç‰¹å¾åˆ†æ":
        enhanced_task2_multidimensional_analysis()
    elif selected_task == "é”€å”®é¢„æµ‹åˆ†æ":
        task3_sales_forecast()
    elif selected_task == "è¿è¥ç­–ç•¥ä¼˜åŒ–":
        task4_operation_optimization()
    elif selected_task == "ç³»ç»ŸçŠ¶æ€":
        show_system_status()

    # é¡µè„š
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: #666;'>"
        "ç”µå•†é”€å”®åˆ†æä¸ç­–ç•¥ä¼˜åŒ–ç³»ç»Ÿ | æ”¯æŒè®ºæ–‡è¦æ±‚çš„æ ‡å‡†åŒ–è¾“å‡º"
        "</div>",
        unsafe_allow_html=True
    )

# ============================================================================
# è¿è¡Œåº”ç”¨
# ============================================================================
if __name__ == "__main__":
    main()
